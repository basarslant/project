## SUPPORT VECTOR MACHINE


#### Introduction to Classification


   The classification problem can be restricted to consideration of the two-class problem without loss of generality. The goal is to separate the two classes by a function which is induced from available examples and it is to produce a classifier that will work well on unseen examples, i.e. it generalises well. 
   
   
*********Page 5 Figure 2.1 and Figure 2.3 : http://users.ecs.soton.ac.uk/srg/publications/pdf/SVM.pdf
http://www.jaist.jp/~h-yamada/pdf/iwpt2003.pdf


   Support Vector Machines are based on the concept of decision planes that define decision boundaries. A decision plane is one that separates between a set of objects having different class memberships. A schematic example is shown in the illustration below. In this example, the objects belong either to class **green** or **red**. The separating line defines a boundary on the right side of which all objects are green and to the left of which all objects are red. Any new object (white circle) falling to the right is labeled, i.e., classified, as green (or classified as RED should it fall to the left of the separating line).

\begin{center}
![](http://www.statsoft.com/textbook/graphics/SVMIntro1.gif)
\end{center}

   The above is a classic example of a linear classifier, i.e., a classifier that separates a set of objects into their respective groups (green and red in this case) with a line. Most classification tasks, however, are not that simple, and often more complex structures are needed in order to make an optimal separation, i.e., correctly classify new objects (test cases) on the basis of the examples that are available (train cases). This situation is depicted in the illustration below. Compared to the previous schematic, it is clear that a full separation of the green and red objects would require a curve (which is more complex than a line). Classification tasks based on drawing separating lines to distinguish between objects of different class memberships are known as hyperplane classifiers. Support Vector Machines are particularly suited to handle such tasks.

\begin{center}
![](http://www.statsoft.com/textbook/graphics/SVMIntro2.gif)
\end{center}

   The illustration below shows the basic idea behind Support Vector Machines. Here we see the original objects (left side of the schematic) mapped, i.e., rearranged, using a set of mathematical functions, known as kernels. The process of rearranging the objects is known as mapping (transformation). Note that in this new setting, the mapped objects (right side of the schematic) is linearly separable and, thus, instead of constructing the complex curve (left schematic), all we have to do is to find an optimal line that can separate the green and the red objects.

\begin{center}
![](http://www.statsoft.com/textbook/graphics/SVMIntro3.gif)
\end{center}

   There are many possible linear classifiers that can separate the data, but there is only one that maximises the **margin** (*maximises the distance between it and the nearest data point of each class*). This linear classifier is termed the optimal separating **hyperplane** which is ***support vector***.  Intuitively, we would expect this boundary to generalise well as opposed to the other possible boundaries.

\begin{center}
![](http://www.saedsayad.com/images/SVM_2.png)
\end{center}

#### Algorithm:

- Define an optimal hyperplane: maximize margin
- Extend the above definition for non-linearly separable problems: have a penalty term for misclassifications.
- Map data to high dimensional space where it is easier to classify with linear decision surfaces: reformulate problem so that data is mapped implicitly to this space.
- To define an optimal hyperplane we need to maximize the width of the margin (w).

\begin{center}
![](http://www.saedsayad.com/images/SVM_optimize.png)
![](http://www.saedsayad.com/images/SVM_optimize_1.png)
\end{center}

>We find w and b by solving the following objective function using Quadratic Programming.

\begin{center}
![](http://www.saedsayad.com/images/SVM_optimize_2.png)
\end{center}

   The beauty of SVM is that if the data is *linearly separable*, there is a unique global minimum value. An ideal SVM analysis should produce a hyperplane that completely separates the vectors (cases) into two non-overlapping classes. However, perfect separation may not be possible, or it may result in a model with so many cases that the model does not classify correctly. In this situation SVM finds the hyperplane that ***maximizes the margin and minimizes the misclassifications***.

#### Kernel Function 

   The simplest way to separate two groups of data is with a straight line (1 dimension), flat plane (2 dimensions) or an N-dimensional hyperplane. However, there are situations where a nonlinear region can separate the groups more efficiently. SVM handles this by using a kernel function (nonlinear) to map the data into a different space where a hyperplane (linear) cannot be used to do the separation. It means a non-linear function is learned by a linear learning machine in a high-dimensional feature space while the capacity of the system is controlled by a parameter that does not depend on the dimensionality of the space. This is called **kernel trick** which means *the kernel function transform the data into a higher dimensional feature space to make it possible to perform the linear separation*.  


\begin{center}
![](http://www.saedsayad.com/images/SVM_4.png)
![](http://www.statsoft.com/textbook/graphics/SVMIntro15.gif)
\end{center}

where **K(X_{i},X_{j}}= \phi(X_{i}). \phi(X_{j})** that is, the kernel function, represents a dot product of input data points mapped into the higher dimensional feature space by transformation.

* Gamma(\gamma) is an adjustable parameter of certain kernel functions.
* *The Radial Basis Function Kernel*, also called the RBF kernel, or Gaussian kernel, is a kernel that is in the form of a radial basis function (more specifically, a Gaussian function).


   Map data into new space, then take the inner product of the new vectors. The image of the inner product of the data is the inner product of the images of the data. Two kernel functions are shown below.
   
\begin{center}
![](http://www.saedsayad.com/images/SVM_kernel_1.png)
\end{center}  
   

##### REFERENCES:

- http://users.ecs.soton.ac.uk/srg/publications/pdf/SVM.pdf
- http://www.jaist.jp/~h-yamada/pdf/iwpt2003.pdf
- http://www.statsoft.com/Textbook/Support-Vector-Machines#overview
- http://www.saedsayad.com/support_vector_machine.htm
