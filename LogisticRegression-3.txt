# LOGISTIC REGRESSION

#### *Introduction to Logistic Regression Model*

   Before beginning a study of logistic regression it is important to understand that the goal of an analysis using this method is the same as that of any model-building technique used in statistics: to find the best fitting and most parsimonious, yet biologically reasonable model to describe the relationship between an outcome variable which is dependent or response and a set of independent variables which is predictor or explanatory. These independent variables are called **covariates**.

   In any regression problem the key quantity is the mean value of the outcome variable, given the value of the independent variable which is called **conditional mean** and it is expressed as **"E(Y|x)"** where *Y* denotes the outcome variable and *x* denotes a value of the independent variable. E(Y|x) is read *"the expected value of Y, given the value x."* In linear regression we assume that this mean may be expressed as an euation linear in x ( or some transformation of *x* or *Y*), such as ***E(Y|x)=\beta_{0} +\beta_{1}x***
This expression implies that it is possible for E(Y|x) to take on any value as x ranges between -\infty and +\infty.


#### *Multiple Logistic Regression Analysis* 

   Consider a collection of p independent variables denoted  by the vector x'=(x_{1},x_{2},...,x_{p}). For the moment we will assume that each of these variables is at least interval scale. Let the conditional probability that the outcome is present be denoted by P(Y=1|x)= \pi(x). The *logit* of the multiple logistic regression model is given by the equation 
                                                                                    \begin{center}
**g(x)=\beta_{0} +\beta_{1}x_{1}+\beta_{2}x_{2}+...+\beta_{p}x_{p}**, 
\end{center}

in which case the logistic regression model is
\begin{center}
\pi(x)=\frac{\mathrm{e}^{g(x)}}{1+\mathrm{e}^{g(x)}}
\end{center}

#### *Interpretetion of Fitting Logistic Regression Analysis*

   The first step is to determine what function of the dependent variable yields a linear function of the independent variables. This is called the ***link function*** [see McCullagh and Nelder(1983) or Dobson(1990)]. In this case of a linear regression model, it is the identity function since the dependent variable, by definition, is linear in the parameters. In the logistic regression model the link function is the logit transformation

\begin{center}
g(x)=\ln{\frac{\pi(x)}{1-\pi(x)}}=\beta_{0} +\beta_{1}x 
\end{center}

****************************************
   In the regression model, the slope coefficient represents the change in the logit corresponding to a change of one unit in the independent variable (i.e. **\beta_{1}= g(x+1)-g(x)**). Proper interpretation of the coefficient in a logistic regression model depends on being able to place meaning on the difference between two logits.


#### Dichotomous Independent Variable
  
   We begin our consideration of the interpretation of logistic regression coefficients with the situation where the independent variable is nominal scale and dichotomous(i.e. measured at two levels). This case provides the conceptual foundation for all the other situations.

   We assume that the independent variable, *x*, is coded as either zero or one The difference in the logit for a subject with x=1 and x=0 is
\begin{center}
g(1)-g(0)=[\beta_{0} +\beta_{1}x]- \beta_{0}=\beta_{1}.
\end{center}

   The algebra shown in this equation is rather straightforward. We present it in this level of detail to emphasize that the fisrt step in interpreting the effect of a covariate in a model is to express the desired logit difference in terms of the model. In this case the logit difference is equal to \beta_{1}. In order to interpret this result we need to introduce and discuss a measure of association termed the **odds ratio**.
	 
   The *odds* of the outcome being present among individuals with x=1 is defined as \frac{\pi(1)}{1-\pi(1)}. Similarly the odds of the outcome being present among individuals with x=0 is defined as \frac{\pi(0)}{1-\pi(0)}. The **odds ratio** is defined as the ratio of the odds for x=1 to the *odds* for x=0 and is given by the equation
\begin{center}
Odds Ratio={\frac{\frac{\pi(1)}{1-\pi(1)}}{\frac{\pi(0)}{1-\pi(0)}}}}
\end{center}

***************
Hence for logistic regression with a dichotomous independent variable coded 1 and 0, the relationship between the odds ratio and the regression coefficient is **Odds Ratio= \mathrm{e}^{\beta_{1}}**.

This simple relationship between the coefficient and the odds ratio is the fundamental reason why logistic regression has proven to be such a powerful analytic research tool.

The interpretation given for the odds ratio is based on the fact that in many instances it approximates a quantity called **the relative risk**. This parameter is equal to the ratio \frac{\pi(1)}{\pi(0)}. It follows that the odds ratio approximates the relative risk \frac{1-\pi(0)}{1-\pi(1)}\approx 1.
This holds when \pi(x) is small for both x=1 and x=0.




**Logistic Regression Model** is a statistical method for analyzing a dataset in which are *dichotomous(binary)* independent variables that determine an outcome. 

What distinguishes a logistic regression from the linear regression model is this. The prediction is based on the use of one or several predictors (numerical and categorical). A linear regression is not appropriate for predicting the value of a binary variable for two reasons:

- A linear regression will predict values outside the acceptable range (e.g. predicting probabilities outside the range 0 to 1)
- Since the dichotomous experiments can only have one of two possible values for each experiment, the residuals will not be normally distributed about the predicted line.
                                                                                       
![](http://www.saedsayad.com/images/LogReg_1.png)

 >On the other hand, a logistic regression produces a logistic curve, which is limited to values between 0 and 1.

  


Logistic regression is similar to a linear regression, but the curve is constructed using the natural logarithm of the “odds” of the target variable, rather than the probability. Moreover, the predictors do not have to be normally distributed or have equal variance in each group.


In the logistic regression the constant (b0) moves the curve left and right and the slope (b1) defines the steepness of the curve. By simple transformation, the logistic regression equation can be written in terms of an odds ratio.

Finally, taking the natural log of both sides, we can write the equation in terms of log-odds (logit) which is a linear function of the predictors. The coefficient (b1) is the amount the logit (log-odds) changes with a one unit change in x. 


As mentioned before, logistic regression can handle any number of numerical and/or categorical variables.

There are several analogies between linear regression and logistic regression. Just as ordinary least square regression is the method used to estimate coefficients for the best fit line in linear regression, logistic regression uses maximum likelihood estimation (MLE) to obtain the model coefficients that relate predictors to the target. After this initial


#### ***RESOURCES:***
- http://resource.heartonline.cn/20150528/1_3kOQSTg.pdf
- http://archive.ics.uci.edu/ml/datasets/Bank+Marketing
- http://www.saedsayad.com/logistic_regression.htm
- https://www.medcalc.org/manual/logistic_regression.php

