{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPARISON OF LOGISTIC REGRESSION AND SUPPORT VECTOR MACHİNE \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### DATA SET INFORMATION\n",
    "\n",
    "- The data comes from direct marketing campaigns of a Portuguese banking institution [1]. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed. \n",
    "\n",
    "- The data has 86520 entries. Each column contains 4119 entry and there are 21 columns which contain client data, contact information, attributes and output variable.\n",
    "\n",
    "\n",
    "### Client Data\n",
    "\n",
    "**1. Age:** This column indicates the age of the customer. It is a numeric variable and ranges between 18 and 88. \t\n",
    "\n",
    "**2. Job:** This column indicates the job of the customer. There are only a few options. These are\n",
    "- 'admin',\n",
    "- 'blue-collar',\n",
    "- 'entrepreneur',\n",
    "- 'housemaid',\n",
    "- 'management',\n",
    "- 'retired',\n",
    "- 'self-employed',\n",
    "- 'services',\n",
    "- 'student',\n",
    "- 'technician',\n",
    "- 'unemployed',\n",
    "- 'unknown'\n",
    "\n",
    "**3. Marital:** This column indicates the marital status of the customer. It is a categorical variable and the options are\n",
    "\n",
    "- 'divorced',\n",
    "- 'married',\n",
    "- 'single',\n",
    "- 'unknown'\n",
    "\n",
    "**4.Education:** This column indicates the education status of the customer. It is a categorical variable and the options are \n",
    "- 'basic.4y',\n",
    "- 'basic.6y',\n",
    "- 'basic.9y',\n",
    "- 'high.school',\n",
    "- 'illiterate',\n",
    "- 'professional.course',\n",
    "- 'university.degree',\n",
    "- 'unknown'\n",
    "\n",
    "**5.Default:** This column indicates that customers have credit in default. It is a categorical variable and there are three options. These are \n",
    "\n",
    "- 'no',\n",
    "- 'yes',\n",
    "- 'unknown'\n",
    "\n",
    "**6.Housing:** This column indicates that customers have housing loan. It is a categorical variable and there are three options. These are \n",
    "\n",
    "- 'no',\n",
    "- 'yes',\n",
    "- 'unknown'\n",
    "\n",
    "**7.Loan:** This column indicates that customers have personal loan. It is a categorical variable and there are three options. These are  \n",
    "\n",
    "- 'no',\n",
    "- 'yes',\n",
    "- 'unknown'\n",
    "\n",
    "\n",
    "### Information Of The Last Contact\n",
    "\n",
    "**8.Contact:** This column indicates the type of communication established with the customer. It is a categorical variable and there are two options. These are \n",
    "\n",
    "- 'cellular',\n",
    "- 'telephone'\n",
    "\n",
    "**9.Month:** This column indicates that month of year which is last contact with the customer. It is a categorical variable and the options are\n",
    "\n",
    "- 'jan', \n",
    "- 'feb',\n",
    "- 'mar',\n",
    "- 'apr',\n",
    "- 'may',\n",
    "- 'jun',\n",
    "- 'jul',\n",
    "- 'aug',\n",
    "- 'sep',\n",
    "- 'oct',\n",
    "- 'nov',\n",
    "- 'dec'\n",
    "\n",
    "**10.Day_of_week:** This column shows that day of week which is last contact with the customer. It is a categorical variable and the options are\n",
    "\n",
    "- 'mon',\n",
    "- 'tue',\n",
    "- 'wed',\n",
    "- 'thu',\n",
    "- 'fri'\n",
    "\n",
    "**11.Duration:** This column indicates that how long the last duration of contact with the customer. It is a numeric variable and ranges between 0 and 3643 seconds. If the duration is 0 ,it means that the output variable is 'no'. Also the duration is not known before a call is performed. After the end of the call, y is obviously known. Thus, we  should only include this input for benchmark purposes.\n",
    "\n",
    "\n",
    "### Other Attributes\n",
    "\n",
    "**12.Campaign:** This column indicates that number of contacts performed during this campaign and for this client. It is a numeric variable and ranges between 1 and 35.\t\n",
    "\n",
    "**13.Pdays:** This column indicates that number of days that passed by after the client was last contacted from a previous campaign. It is a numeric variable and ranges between 0 and 999. Also 999 means that client was not previously contacted.\t\n",
    "\n",
    "**14.Previous:** This column indicates that number of contacts performed before this campaign and for this client. It is a numeric variable and ranges between 0 and 6.\n",
    "\n",
    "**15.Poutcome:** This column indicates that outcome of the previous marketing campaign. It is a categorical variable and the options are   \n",
    "\n",
    "- 'failure',\n",
    "- 'nonexistent',\n",
    "- 'success'\n",
    "\n",
    "\n",
    "### Social And Economic Context Attributes \n",
    "\n",
    "**16.Emp.var.rate:** This column indicates that employment variation rate. It is a numeric variable and ranges between -3 and 1.4. In addition this rate came from quarterly indicator.\t\n",
    "\n",
    "**17.Cons.price.idx:** This column indicates that consumer price index. It is a numeric variable and ranges between 92201 and 94767. In addition this index come from monthly indicator.\n",
    "\n",
    "**18.Cons.conf.idx:** This column indicates that consumer confidence index. It is a numeric variable and ranges between -50 and -33. In addition this index come from monthly indicator.\t\n",
    "\n",
    "**19.Euribor3m:** This column indicates that Euribor 3 month rate which come daily indicator. It is a numeric variable and ranges between 1 and 5045.\n",
    "\n",
    "**20.Nr.employed:** This column indicates that number of employees which come quarterly indicator. It is also a numeric variable and ranges between 5191 and 5228.1. \n",
    "\n",
    "### Output Variable \n",
    "\n",
    "**21.Y:** This column indicates that has the client subscribed a term deposit. It is a binary variable and the options are \n",
    "\n",
    "- 'yes',\n",
    "- 'no'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **OVERVIEW OF THE DATA**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Data = (4119, 21)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>...</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45</td>\n",
       "      <td>entrepreneur</td>\n",
       "      <td>married</td>\n",
       "      <td>university.degree</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>cellular</td>\n",
       "      <td>aug</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.4</td>\n",
       "      <td>93444.0</td>\n",
       "      <td>-36.1</td>\n",
       "      <td>4965.0</td>\n",
       "      <td>5228.1</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "      <td>technician</td>\n",
       "      <td>single</td>\n",
       "      <td>university.degree</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>cellular</td>\n",
       "      <td>aug</td>\n",
       "      <td>wed</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.4</td>\n",
       "      <td>93444.0</td>\n",
       "      <td>-36.1</td>\n",
       "      <td>4967.0</td>\n",
       "      <td>5228.1</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>cellular</td>\n",
       "      <td>aug</td>\n",
       "      <td>wed</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.4</td>\n",
       "      <td>93444.0</td>\n",
       "      <td>-36.1</td>\n",
       "      <td>4965.0</td>\n",
       "      <td>5228.1</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38</td>\n",
       "      <td>technician</td>\n",
       "      <td>married</td>\n",
       "      <td>professional.course</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>aug</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.4</td>\n",
       "      <td>93444.0</td>\n",
       "      <td>-36.1</td>\n",
       "      <td>4965.0</td>\n",
       "      <td>5228.1</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>university.degree</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>aug</td>\n",
       "      <td>tue</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.4</td>\n",
       "      <td>93444.0</td>\n",
       "      <td>-36.1</td>\n",
       "      <td>4963.0</td>\n",
       "      <td>5228.1</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age           job  marital            education  default housing loan  \\\n",
       "0   45  entrepreneur  married    university.degree  unknown     yes  yes   \n",
       "1   29    technician   single    university.degree       no     yes  yes   \n",
       "2   40    management  married          high.school       no      no  yes   \n",
       "3   38    technician  married  professional.course       no     yes   no   \n",
       "4   34        admin.  married    university.degree       no      no   no   \n",
       "\n",
       "    contact month day_of_week ...   campaign  pdays  previous     poutcome  \\\n",
       "0  cellular   aug         mon ...          2    999         0  nonexistent   \n",
       "1  cellular   aug         wed ...          3    999         0  nonexistent   \n",
       "2  cellular   aug         wed ...          1    999         0  nonexistent   \n",
       "3  cellular   aug         mon ...          1    999         0  nonexistent   \n",
       "4  cellular   aug         tue ...          1    999         0  nonexistent   \n",
       "\n",
       "  emp.var.rate  cons.price.idx  cons.conf.idx  euribor3m  nr.employed    y  \n",
       "0          1.4         93444.0          -36.1     4965.0       5228.1   no  \n",
       "1          1.4         93444.0          -36.1     4967.0       5228.1   no  \n",
       "2          1.4         93444.0          -36.1     4965.0       5228.1   no  \n",
       "3          1.4         93444.0          -36.1     4965.0       5228.1  yes  \n",
       "4          1.4         93444.0          -36.1     4963.0       5228.1   no  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset=pd.read_csv('Data-main.csv',header=0)\n",
    "print(\"Size of Data =\", dataset.shape)\n",
    "dataset=dataset.dropna()\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially we construct data for *.head()* code and we see the first six columns of the data and we see the size of data with *.shape()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               0\n",
       "job               0\n",
       "marital           0\n",
       "education         0\n",
       "default           0\n",
       "housing           0\n",
       "loan              0\n",
       "contact           0\n",
       "month             0\n",
       "day_of_week       0\n",
       "duration          0\n",
       "campaign          0\n",
       "pdays             0\n",
       "previous          0\n",
       "poutcome          0\n",
       "emp.var.rate      0\n",
       "cons.price.idx    0\n",
       "cons.conf.idx     0\n",
       "euribor3m         0\n",
       "nr.employed       0\n",
       "y                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If any statistical method is applied, successful operation can not be obtained if there are deleted or damaged cells in the data.\n",
    "\n",
    "It seems that there is no undefined value or null cell in data. This means that we can efficiently complete the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEICAYAAACwDehOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUJAkNYaCJKkxFCRJzf8Hywhpb8UDYiYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c3d6780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_name = ('Yes','No')\n",
    "dataset['y_new']=dataset.y.map({'yes':1, 'no':0})\n",
    "incomes = (0.1094926,0.8905074)\n",
    "y_post = np.arange(len(y_name))\n",
    "plt.bar( y_post,incomes,align='center',alpha=0.7)\n",
    "plt.xticks(y_post,y_name)\n",
    "plt.ylabel('Counts')\n",
    "plt.title('Independent Variable')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here y values are seen to be unbalanced. \n",
    "\n",
    "*451 of the customer*, which corresponds to approximately **11%**, gave a \"Yes\" answer when the customer was asked to sign up for a time deposit and *3668 of the customer*, which corresponds to approximately **89%**, gave \"No\" reply when the customer was asked to sign up for a time deposit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### PREPARE THE DATA FOR ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy Variables=\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, ..., 0, 1, 0],\n",
       "       [0, 1, 0, ..., 0, 1, 0],\n",
       "       [0, 1, 0, ..., 0, 1, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0]], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.drop(['marital','job','education','y'],1, inplace=True)\n",
    "dataset['default'] = dataset['default'].astype('category')\n",
    "dataset['housing'] = dataset['housing'].astype('category')\n",
    "dataset['loan'] = dataset['loan'].astype('category')\n",
    "dataset['contact'] = dataset['contact'].astype('category')\n",
    "dataset['month'] = dataset['month'].astype('category')\n",
    "dataset['day_of_week'] = dataset['day_of_week'].astype('category')\n",
    "dataset['poutcome'] = dataset['poutcome'].astype('category')\n",
    "\n",
    "categoricals = list(dataset.dtypes[dataset.dtypes == 'category'].index)\n",
    "columnnames = list(dataset)\n",
    "\n",
    "datasetNumerics = dataset.drop(categoricals,1)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "classes = []\n",
    "for i in categoricals:\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    dataset[i] = le.fit_transform(dataset[i].as_matrix())\n",
    "    classes.append(list(le.classes_))\n",
    "    \n",
    "enc=preprocessing.OneHotEncoder(dtype=np.int32,sparse=False)\n",
    "X = enc.fit_transform(dataset[categoricals])\n",
    "unique = dataset[categoricals].apply(lambda x: x.value_counts()).unstack()\n",
    "unique = unique[~unique.isnull()]\n",
    "dataset[categoricals].head()\n",
    "enc_cols = list(unique.index.map('{0[0]}_{0[1]}'.format))\n",
    "\n",
    "datasetCategoricals = pd.DataFrame(X, columns=enc_cols, index=dataset[categoricals].index)\n",
    "dataset = datasetNumerics.merge(datasetCategoricals,left_index = True, right_index = True)\n",
    "\n",
    "y = dataset['y_new']\n",
    "del dataset['y_new']\n",
    "X = dataset\n",
    "\n",
    "print(\"Dummy Variables=\")\n",
    "dataset.iloc[:,21:].values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our independent variables are composed of many different categorical values. For this reason, we are creating **dummy variables** that we code as 0 and 1 in order to complete to apply the methods in the python. Then using *Encoder and OneHotEncoder*, dummy variables have been created.\n",
    "\n",
    "A **one hot encoding** is a representation of categorical variables as binary vectors. This first requires that the categorical values be mapped to integer values. Then, each integer value is represented as a binary vector that is all zero values except the index of the integer, which is marked with a 1.\n",
    "\n",
    "One hot encoding allows the representation of categorical data to be more expressive. Many machine learning algorithms cannot work with categorical data directly. The categories must be converted into numbers. This is required for both input and output variables that are categorical. When a one hot encoding is used for the output variable, it may offer a more nuanced set of predictions than a single label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>...</th>\n",
       "      <th>month_8</th>\n",
       "      <th>month_9</th>\n",
       "      <th>day_of_week_0</th>\n",
       "      <th>day_of_week_1</th>\n",
       "      <th>day_of_week_2</th>\n",
       "      <th>day_of_week_3</th>\n",
       "      <th>day_of_week_4</th>\n",
       "      <th>poutcome_0</th>\n",
       "      <th>poutcome_1</th>\n",
       "      <th>poutcome_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4119.000000</td>\n",
       "      <td>4119.000000</td>\n",
       "      <td>4119.000000</td>\n",
       "      <td>4119.000000</td>\n",
       "      <td>4119.000000</td>\n",
       "      <td>4119.000000</td>\n",
       "      <td>4119.000000</td>\n",
       "      <td>4119.000000</td>\n",
       "      <td>4119.000000</td>\n",
       "      <td>4119.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4119.000000</td>\n",
       "      <td>4119.000000</td>\n",
       "      <td>4119.000000</td>\n",
       "      <td>4119.000000</td>\n",
       "      <td>4119.000000</td>\n",
       "      <td>4119.000000</td>\n",
       "      <td>4119.000000</td>\n",
       "      <td>4119.000000</td>\n",
       "      <td>4119.000000</td>\n",
       "      <td>4119.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>40.113620</td>\n",
       "      <td>256.788055</td>\n",
       "      <td>2.537266</td>\n",
       "      <td>960.422190</td>\n",
       "      <td>0.190337</td>\n",
       "      <td>0.084972</td>\n",
       "      <td>84854.473707</td>\n",
       "      <td>-40.499102</td>\n",
       "      <td>3244.553921</td>\n",
       "      <td>5166.481695</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016752</td>\n",
       "      <td>0.015538</td>\n",
       "      <td>0.186453</td>\n",
       "      <td>0.207575</td>\n",
       "      <td>0.208789</td>\n",
       "      <td>0.204176</td>\n",
       "      <td>0.193008</td>\n",
       "      <td>0.110221</td>\n",
       "      <td>0.855305</td>\n",
       "      <td>0.034474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10.313362</td>\n",
       "      <td>254.703736</td>\n",
       "      <td>2.568159</td>\n",
       "      <td>191.922786</td>\n",
       "      <td>0.541788</td>\n",
       "      <td>1.563114</td>\n",
       "      <td>27265.192237</td>\n",
       "      <td>4.594578</td>\n",
       "      <td>1995.780101</td>\n",
       "      <td>73.667904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.128355</td>\n",
       "      <td>0.123693</td>\n",
       "      <td>0.389519</td>\n",
       "      <td>0.405620</td>\n",
       "      <td>0.406492</td>\n",
       "      <td>0.403147</td>\n",
       "      <td>0.394707</td>\n",
       "      <td>0.313203</td>\n",
       "      <td>0.351836</td>\n",
       "      <td>0.182466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.400000</td>\n",
       "      <td>93.200000</td>\n",
       "      <td>-50.800000</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>4963.600000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>32.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.800000</td>\n",
       "      <td>92893.000000</td>\n",
       "      <td>-42.700000</td>\n",
       "      <td>1281.000000</td>\n",
       "      <td>5099.100000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>38.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>93749.000000</td>\n",
       "      <td>-41.800000</td>\n",
       "      <td>4856.000000</td>\n",
       "      <td>5191.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>47.000000</td>\n",
       "      <td>317.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>93994.000000</td>\n",
       "      <td>-36.400000</td>\n",
       "      <td>4961.000000</td>\n",
       "      <td>5228.100000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.000000</td>\n",
       "      <td>3643.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>94767.000000</td>\n",
       "      <td>-26.900000</td>\n",
       "      <td>5045.000000</td>\n",
       "      <td>5228.100000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               age     duration     campaign        pdays     previous  \\\n",
       "count  4119.000000  4119.000000  4119.000000  4119.000000  4119.000000   \n",
       "mean     40.113620   256.788055     2.537266   960.422190     0.190337   \n",
       "std      10.313362   254.703736     2.568159   191.922786     0.541788   \n",
       "min      18.000000     0.000000     1.000000     0.000000     0.000000   \n",
       "25%      32.000000   103.000000     1.000000   999.000000     0.000000   \n",
       "50%      38.000000   181.000000     2.000000   999.000000     0.000000   \n",
       "75%      47.000000   317.000000     3.000000   999.000000     0.000000   \n",
       "max      88.000000  3643.000000    35.000000   999.000000     6.000000   \n",
       "\n",
       "       emp.var.rate  cons.price.idx  cons.conf.idx    euribor3m  nr.employed  \\\n",
       "count   4119.000000     4119.000000    4119.000000  4119.000000  4119.000000   \n",
       "mean       0.084972    84854.473707     -40.499102  3244.553921  5166.481695   \n",
       "std        1.563114    27265.192237       4.594578  1995.780101    73.667904   \n",
       "min       -3.400000       93.200000     -50.800000     0.640000  4963.600000   \n",
       "25%       -1.800000    92893.000000     -42.700000  1281.000000  5099.100000   \n",
       "50%        1.100000    93749.000000     -41.800000  4856.000000  5191.000000   \n",
       "75%        1.400000    93994.000000     -36.400000  4961.000000  5228.100000   \n",
       "max        1.400000    94767.000000     -26.900000  5045.000000  5228.100000   \n",
       "\n",
       "          ...           month_8      month_9  day_of_week_0  day_of_week_1  \\\n",
       "count     ...       4119.000000  4119.000000    4119.000000    4119.000000   \n",
       "mean      ...          0.016752     0.015538       0.186453       0.207575   \n",
       "std       ...          0.128355     0.123693       0.389519       0.405620   \n",
       "min       ...          0.000000     0.000000       0.000000       0.000000   \n",
       "25%       ...          0.000000     0.000000       0.000000       0.000000   \n",
       "50%       ...          0.000000     0.000000       0.000000       0.000000   \n",
       "75%       ...          0.000000     0.000000       0.000000       0.000000   \n",
       "max       ...          1.000000     1.000000       1.000000       1.000000   \n",
       "\n",
       "       day_of_week_2  day_of_week_3  day_of_week_4   poutcome_0   poutcome_1  \\\n",
       "count    4119.000000    4119.000000    4119.000000  4119.000000  4119.000000   \n",
       "mean        0.208789       0.204176       0.193008     0.110221     0.855305   \n",
       "std         0.406492       0.403147       0.394707     0.313203     0.351836   \n",
       "min         0.000000       0.000000       0.000000     0.000000     0.000000   \n",
       "25%         0.000000       0.000000       0.000000     0.000000     1.000000   \n",
       "50%         0.000000       0.000000       0.000000     0.000000     1.000000   \n",
       "75%         0.000000       0.000000       0.000000     0.000000     1.000000   \n",
       "max         1.000000       1.000000       1.000000     1.000000     1.000000   \n",
       "\n",
       "        poutcome_2  \n",
       "count  4119.000000  \n",
       "mean      0.034474  \n",
       "std       0.182466  \n",
       "min       0.000000  \n",
       "25%       0.000000  \n",
       "50%       0.000000  \n",
       "75%       0.000000  \n",
       "max       1.000000  \n",
       "\n",
       "[8 rows x 39 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we look at the describing information of numerical columns. It is obviously numerical column values are not close each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>y_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.385714</td>\n",
       "      <td>0.010431</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.986026</td>\n",
       "      <td>0.615063</td>\n",
       "      <td>0.984141</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.171836</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.986026</td>\n",
       "      <td>0.615063</td>\n",
       "      <td>0.984537</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.314286</td>\n",
       "      <td>0.032665</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.986026</td>\n",
       "      <td>0.615063</td>\n",
       "      <td>0.984141</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.131485</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.986026</td>\n",
       "      <td>0.615063</td>\n",
       "      <td>0.984141</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.228571</td>\n",
       "      <td>0.029920</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.986026</td>\n",
       "      <td>0.615063</td>\n",
       "      <td>0.983744</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age  duration  campaign  pdays  previous  emp.var.rate  \\\n",
       "0  0.385714  0.010431  0.029412    1.0       0.0           1.0   \n",
       "1  0.157143  0.171836  0.058824    1.0       0.0           1.0   \n",
       "2  0.314286  0.032665  0.000000    1.0       0.0           1.0   \n",
       "3  0.285714  0.131485  0.000000    1.0       0.0           1.0   \n",
       "4  0.228571  0.029920  0.000000    1.0       0.0           1.0   \n",
       "\n",
       "   cons.price.idx  cons.conf.idx  euribor3m  nr.employed  y_new  \n",
       "0        0.986026       0.615063   0.984141          1.0    0.0  \n",
       "1        0.986026       0.615063   0.984537          1.0    0.0  \n",
       "2        0.986026       0.615063   0.984141          1.0    0.0  \n",
       "3        0.986026       0.615063   0.984141          1.0    1.0  \n",
       "4        0.986026       0.615063   0.983744          1.0    0.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset =(datasetNumerics - datasetNumerics.min()) / (datasetNumerics.max()- datasetNumerics.min())\n",
    "Dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative approach to Z-score normalization (or standardization) is the so-called Min-Max scaling (often also simply called “normalization” - a common cause for ambiguities).\n",
    "In this approach, the data is scaled to a fixed range - usually 0 to 1.\n",
    "The cost of having this bounded range - in contrast to standardization - is that we will end up with smaller standard deviations, which can suppress the effect of outliers.\n",
    "\n",
    "A Min-Max scaling is typically done via the following equation: $$ \\frac{x-x_{min}}{x_{max}-x_{min}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train and test values: (3295, 39) (824, 39) (3295,) (824,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2 , random_state=25)\n",
    "print(\"train and test values:\",X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We divide the data to constitute out model and we divide **20%** of the data randomly with random state=25.\n",
    "\n",
    "*824* sample of data is test variables and *3295* sample of data is train variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# LOGISTIC REGRESSION\n",
    "\n",
    "### INTRODUCTION TO LOGISTIC REGRESSION METHOD\n",
    "                                                                        \n",
    "*Logistic Regression* is a simple machine learning method for analyzing a dataset you can use to predict the value of a numeric categorical variable based on its relationship with predictor variables. Like all other statistical methods,the main purpose here is the best fitting and biologically reasonable model to describe the relationship. \n",
    "\n",
    "There is an *Outcome variable* which is called that *dependent* or *response* and also there are *independent variables* that are called that *predictor* or *explanatory*.\n",
    "\n",
    "In regression problems, \"*E (Y | x) *\" is called the conditional average and gives the average of the result variable. *Y denotes the outcome variable* and *x denotes a value of the independent variable. E(Y|x) is read *\"the expected value of Y, given the value x.\"*\n",
    "\n",
    "In the *linear regression*, we assume that this average can be expressed linearly (or some transformations of x or Y) in x such as $ E (Y | x) = \\beta_ {0} + \\beta_ {1}x+ \\epsilon_{i} $ .\n",
    "\n",
    "$ \\epsilon _{i} $ , is called the error term. This variable captures all other factors which influence the dependent variable $ y_{i} $ other than the regressors $ x_{i} $.  \n",
    "This implies that it is possible for E (Y | x) to take any value, such as x intervals between $ - \\infy $ and $ \\infy $ .\n",
    "\n",
    "   \n",
    "What distinguishes a logistic regression from the linear regression model is using several predictors which are numerical and categorical. A linear regression is not appropriate for predicting the value of a binary variable for two reasons:\n",
    "\n",
    "-  A linear regression will predict values outside the acceptable range 0 to 1.\n",
    "\n",
    "- The dichotomous experiments can only have one of two possible values for each experiment, therefore the residuals will not be normally distributed about the predicted line.\n",
    "\n",
    "![](http://www.saedsayad.com/images/LogReg_1.png)\n",
    "\n",
    "> On the other hand, a logistic regression produces a logistic curve, which is limited to values between 0 and 1. The curve is said to be S-shaped.\n",
    "In the logistic regression the constant (b0) moves the curve left and right and the slope (b1) defines the steepness of the curve. By simple transformation, the logistic regression equation can be written in terms of an odds ratio.\n",
    "\n",
    "> Finally, taking the natural log of both sides, we can write the equation in terms of log-odds (logit) which is a linear function of the predictors. The coefficient (b1) is the amount the logit (log-odds) changes with a one unit change in x.     *** [2] ***\n",
    "\n",
    "\n",
    "### INTERPRETATION OF FITTING LOGISTIC REGRESSION ANALYSIS \n",
    "\n",
    "*A link function* is simply a function of the mean of the response variable Y that we use as the response instead of Y itself. In this case, the linear regression model is an identification function because it is linear in the dependent variable parameters required by the definition.\n",
    "\n",
    "In order to simplify notation, when the logistic distribution is used we use the quantity $ \\pi(x)= E (Y | x) $.The specific form of the logistic regression model that:\n",
    "\n",
    "$$ \\pi(x)= \\frac{e^{\\beta_{0} +\\beta_{1}x}}{1+e^{\\beta_{0} +\\beta_{1}x}} $$\n",
    "\n",
    "Then there are three more important description that need to be mentioned,*\"odds\"*, *\"odds ratio\"* and *\"logit transformation\"*:\n",
    "\n",
    "- The **odds** of the dependent variable equaling a case (given some linear combination x of the predictors) is equivalent to the exponential function of the linear regression expression.\n",
    "\n",
    "$$ odds= e^{\\beta_{0} +\\beta_{1}x} $$\n",
    "\n",
    "- Then **odds ratio** occurs like that\n",
    "\n",
    "$$ OR=\\frac{odds(x+1)}{odds(x)}=\\frac{e^{\\beta_{0} +\\beta_{1}x}}{1+e^{\\beta_{0} +\\beta_{1}(x+1)}}= e^{\\beta_{1}}$$\n",
    "\n",
    "- Also in the logistic regression model the *link function* is the **logit transformation** which is \n",
    "\n",
    "$$ g(x)=\\ln{\\frac{\\pi(x)}{1-\\pi(x)}}=\\beta_{0} +\\beta_{1}x $$.\n",
    "\n",
    "\n",
    "In the regression model, the slope coefficient represents the change in the logit corresponding to a change of one unit in the independent variable (i.e. $\\beta_{1}= g(x+1)-g(x)$).\n",
    "\n",
    "To be able to interpret the coefficient properly in a logistic regression model depends on the correct meaning of the difference between the two logits. The interpretation of this distinction is examined in detail on a case-by-case basis, since it is directly dependent on the definition and meaning of an independent unit change.\n",
    "\n",
    "\n",
    "### DICHOTOMOUS INDEPENDENT VARIABLE\n",
    "\n",
    "\n",
    "When interpreting the logistic regression coefficients, it should be taken into account that the independent variable is nominal scale and binary (ie measured at two levels). \n",
    "\n",
    "We assume that the argument is encoded as x or zero. The logit difference of a subject with X = 1 and x = 0 would be:\n",
    "\n",
    "$$ g (1)-g (0) = [\\beta_ {0} + \\beta_ {1} x] - \\beta_ {0} = \\beta_ {1}. $$\n",
    "\n",
    "\n",
    "The odds of the outcome being present among individuals with $x=1$ is defined as $\\frac{\\pi(1)}{1-\\pi(1)}$. Similarly the odds of the outcome being present among individuals with $x=0$ is defined as $\\frac{\\pi(0)}{1-\\pi(0)}$. The odds ratio is defined as the ratio of the odds for $x=1$ to the odds for $x=0$ and is given by the equation\n",
    "\n",
    "$$\n",
    "\\text{Odds Ratio} = \\frac{\\frac{\\pi(1)}{1-\\pi(1)}}{\\frac{\\pi(0)}{1-\\pi(0)}}\n",
    "$$\n",
    "\n",
    "Hence for logistic regression with a dichotomous independent variable coded 1 and 0, the relationship between the odds ratio and the regression coefficient is Odds Ratio$= \\mathrm{e}^{\\beta_{1}}$.\n",
    "This simple relationship between the coefficient and the odds ratio is the fundamental reason why logistic regression has proven to be such a powerful analytic research tool.\n",
    "\n",
    "The interpretation given for the odds ratio is based on the fact that in many instances it approximates a quantity called the relative risk. This parameter is equal to the ratio $\\frac{\\pi(1)}{\\pi(0)}$. It follows that the odds ratio approximates the relative risk $\\frac{1-\\pi(0)}{1-\\pi(1)}\\approx 1$. This holds when $\\pi(x)$ is small for both $x=1$ and $x=0$.\n",
    "\n",
    "\n",
    "### POLYTOMOUS (MULTINOMIAL) LOGISTIC REGRESSION ANALYSIS\n",
    "\n",
    "\n",
    "Where the response is a binary variable with 'success' and 'failure' being only two categories, logistic regression can be applied. But logistic regression can be extended to handle responses, Y, that are polytomous, i.e. taking $ r>2 $ categories.\n",
    "\n",
    "When $ r > 2 $, we have a multi-category or polytomous response variable. There are $r (r − 1)/2$ logits (odds) that we can form, but only $(r − 1)$ are non-redundant. There are different ways to form a set of $(r − 1)$ non-redundant logits, and these will lead to different polytomous (multinomial) logistic regression models.\n",
    "\n",
    "Multinomial Logistic Regression models how multinomial response variable Y depends on a set of k explanatory variables $x=(x_{1},x_{2},...,x_{p})$. This is also a *Generalized Linear Model* where the random component assumes that the distribution of Y is Multinomial(n,π), where π is a vector with probabilities of \"success\" for each category. The systematic component are explanatory variables (can be continuous, discrete, or both) and are linear in the parameters, e.g.,\n",
    "\n",
    "$$ \\beta_{0} +\\beta_{1}x_{1}+\\beta_{2}x_{2}+...+\\beta_{p}x_{p} $$. \n",
    "\n",
    "Again, transformation of the X's themselves are allowed like in linear regression. The link function is the generalized Logit, that it the logit link for each pair of non-redundant logits as discussed above.\n",
    "\n",
    "For the moment we will assume that each of these variables is at least interval scale. Let the conditional probability that the outcome is present be denoted by $P(Y=1|x)= \\pi(x)$. The logit of the multiple logistic regression model is given by the equation\n",
    "\n",
    "$$ g(x)=\\beta_{0} +\\beta_{1}x_{1}+\\beta_{2}x_{2}+...+\\beta_{p}x_{p} $$\n",
    "\n",
    "in which case the logistic regression model is\n",
    "\n",
    "$$ \\pi(x)=\\frac{\\mathrm{e}^{g(x)}}{1+\\mathrm{e}^{g(x)}} $$\n",
    "\n",
    "The multiple logistic regression model can be written as follows:\n",
    "\n",
    "$$\n",
    "\\hat{p} = \\frac{\\beta_{0} +\\beta_{1}x_{1}+\\beta_{2}x_{2}+...+\\beta_{p}x_{p}}{1-\\beta_{0} +\\beta_{1}x_{1}+\\beta_{2}x_{2}+...+\\beta_{p}x_{p}}\n",
    "$$\n",
    "\n",
    "\\hat{p} is the expected probability that the outcome is present; x{1} through x{p} are distinct independent variables; and \\beta{0} through \\beta{p} are the regression coefficients. The multiple logistic regression model is sometimes written differently. In the following form, the outcome is the expected log of the odds that the outcome is present,\n",
    "\n",
    "$$ \\ln{\\frac{\\hat{p}}{1-\\hat{p}}}=\\beta_{0} +\\beta_{1}x_{1}+\\beta_{2}x_{2}+...+\\beta_{p}x_{p} $$\n",
    "\n",
    "When analyzing a polytomous response, it's important to note whether the response is ordinal (consisting of ordered categories) or nominal (consisting of unordered categories). For binary logistic model this question does not arise.\n",
    "\n",
    "Some types of models are appropriate only for ordinal responses; e.g., cumulative logits model, adjacent categories model, continuation ratios model and other models may be used whether the response is ordinal or nominal; e.g., baseline logit model, and conditional logit model.\n",
    "\n",
    "If the response is ordinal, we do not necessarily have to take the ordering into account, but only very rarely this information is ignored. Ordinality in the response is a vital information; neglecting it almost always will lead to sub-optimal models. Using the natural ordering can\n",
    "\n",
    "- lead to a simpler, more parsimonious model \n",
    "- increase power to detect relationships with other variables.\n",
    "\n",
    "If the response variable is polytomous and all the potential predictors are discrete as well, we could describe the multi-way contingency table by a *loglinear model.* However, if you are analyzing a set of categorical variables, and one of them is clearly a \"response\" while the others are predictors, I recommend that you use logistic rather than loglinear models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APPLY THE LOGISTIC REGRESSION METHOD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***Construct The Model:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "model=logreg.fit(X_train, y_train)\n",
    "predictions = logreg.predict(X_test)\n",
    "print (model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression model is formed by using *LogisticRegression()* from the library *sklearn* and then we construct the model with *fit()* and *predict()* code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***Confusion Matrix:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR2FHwMBTqMRw5e2BDbIYDff7scoi4W6I8BvaHLO4_9oUpwHwGX-w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **True Positives (TP):** we correctly predicted that customers do not want to buy deposits.\n",
    "\n",
    "- **True Negatives (TN):** we correctly predicted that customers want to buy deposits.\n",
    "\n",
    "- **False Positives (FP):** we incorrectly predicted that customers do not want to buy deposits.\n",
    "\n",
    "- **False Negatives (FN):** we incorrectly predicted that customers want to buy deposits.\n",
    "\n",
    "*0: negative class*\n",
    "*1: positive class*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[728  20]\n",
      " [ 42  34]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, predictions)\n",
    "print(\"Confusion Matrix: \")\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the first row , **748 customers** do not want to buy deposits (No answer)\n",
    "\n",
    "- We correctly predicted that *728 of them do not want to buy the deposit so customers gave answer \"No\"* . \n",
    "- We incorrectly predicted that *20 of them want to buy the deposit, customers gave answer \"Yes*. But in real customers gave answer \"No\". \n",
    "\n",
    "From the second row , **76 customers** want to buy deposits (Yes answer)\n",
    "\n",
    "- We incorrectly predicted that *42 of them do not want to buy the deposit, customers gave answer \"No\"* . But in real customers gave answer \"Yes\".\n",
    "- We correctly predicted that *34 of them want to buy the deposit so customers gave answer \"Yes*. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *** Classification Report:***\n",
    "\n",
    "In binary classification, **precision **(also called positive predictive value) is the fraction of relevant instances among the retrieved instances, while **recall** (also known as sensitivity) is the fraction of relevant instances that have been retrieved over the total amount of relevant instances. Both precision and recall are therefore based on an understanding and measure of relevance.\n",
    "\n",
    "**Precision:**\n",
    "\n",
    "- **0:** $ \\frac{TN}{(TN+FN)} $\n",
    "- **1:** $ \\frac{TP}{(TP+FP)} $\n",
    "\n",
    "**Recall:**\n",
    "\n",
    "- **0:** $ \\frac{TN}{(TN+FP)} $  \n",
    "- **1:** $ \\frac{TP}{(TP+FN)} $\n",
    "\n",
    "**F1-Score:**\n",
    "\n",
    "- F= $ \\frac{{2}.{precision}.{recall}}{{precision}+{recall}} $ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.97      0.96       748\n",
      "          1       0.63      0.45      0.52        76\n",
      "\n",
      "avg / total       0.92      0.92      0.92       824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Precision, Recall and F1-Score values are too high for answer \"Yes\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression score= 0.92475728\n"
     ]
    }
   ],
   "source": [
    "logreg_score = logreg.predict(X_test)\n",
    "print('Accuracy of logistic regression score= {:.8f}'.format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of logistic regression score is consistent the classification report. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUPPORT VECTOR MACHINE\n",
    "\n",
    "\n",
    "\n",
    "### INTRODUCTION TO CLASSIFICATION\n",
    "\n",
    "Support Vector Machine (SVM) is an audited machine learning algorithm that can be used for classification or regression difficulties, but is often used in classification problems.\n",
    "\n",
    "The classification problem can be restricted to consideration of the two-class problem without loss of generality. The goal is to separate the two classes by a function which is induced from available examples and it is to produce a classifier that will work well on unseen examples, i.e. it generalises well.\n",
    "\n",
    "\n",
    "A Support Vector Machine (SVM) performs classification by finding the hyperplane that maximizes the margin between the two classes. The vectors (cases) that define the hyperplane are the support vectors.\n",
    "\n",
    "![](http://www.saedsayad.com/images/SVM_2.png)$ [4] $\n",
    "\n",
    "  \n",
    "### ALGORITHM:\n",
    "\n",
    "- Define an optimal hyperplane: maximize margin\n",
    "- Extend the above definition for non-linearly separable problems: have a penalty term for misclassifications.\n",
    "- Map data to high dimensional space where it is easier to classify with linear decision surfaces: reformulate problem so that data is mapped implicitly to this space.\n",
    "- To define an optimal hyperplane we need to maximize the width of the margin (w).\n",
    "\n",
    "![]( https://s3.amazonaws.com/quantstart/media/images/qs-svm-0003.png ) [5]\n",
    "\n",
    "\n",
    "Consider the problem of separating the set of training vectors belonging to two separate classes,\n",
    "\n",
    " $$ D= { (x^{1},y^{1}),..., (x^{k},y^{k}) } $$\n",
    " \n",
    " where  $ x \\epsilon R $ , $ y \\epsilon -1 $ and $ y \\epsilon 1 $  with a hyperplane $ < w,x> +b=0 $\n",
    "\n",
    "The set of vectors is said to be optimally separated by the hyperplane if it is separated without error and the distance between the closest vector to the hyperplane is maximal. \n",
    "\n",
    "$$ min| <w,x^{i}>+b | =1 $$\n",
    "\n",
    "This incisive constraint on the parameterisation is preferable to alternatives in simplifying the formulation of the problem. In words it states that: the norm of the weight vector should be equal to the inverse of the distance, of the nearest point in the data set to the hyperplane.\n",
    "\n",
    " SVMs find a hyperplane  $ w · x + b = 0 $ which correctly separates training examples and has maximum margin which is the distance between two hyperplanes $ w · x + b ≥ 1 $ and $ w · x + b ≤ −1 $ \n",
    " \n",
    "A separating hyperplane in canonical form must satisfy the following constraints,\n",
    "\n",
    "$$ y^{i}[<w,x^{i}>+b]\\geq 1 $$ where i=1,...,k $ (*) $\n",
    "\n",
    "The distance d(w, b; x) of a point x from the hyperplane (w, b) is,\n",
    "$$ d(w,b;x)=\\frac{| <w,x^{i}>+b |}{\\parallel w \\parallel} $$ \n",
    "\n",
    "The optimal hyperplane is given by maximising the margin, ρ, subject to the constraints of $ (*) $. The margin is given by,\n",
    "\n",
    "$$  ρ (w,b)= min_{x^{i}:y^{i}=-1} d(w, b; x^{i}) + min_{x^{i}:y^{i}=1} d(w, b; x^{i})=\\frac {2}{\\parallel w \\parallel} $$\n",
    "\n",
    "Hence the hyperplane that optimally separates the data is the one that minimises\n",
    "\n",
    "$$ \\phi (w)=\\frac{1}{2} {\\parallel w \\parallel} ^2 $$ \n",
    "$ (**)$\n",
    "\n",
    "The solution to the optimisation problem of $ (**) $ under the constraints $ (*) $ is given by the saddle point of the Lagrange functional \n",
    "$$ \\phi(w,b,\\alpha)=\\frac{1}{2}{\\parallel w \\parallel}^2-\\sum_i^l\\alpha_{i}(y^{i}[<w,x^{i}>+b]-1) $$ , where i=1,2,...l\n",
    "and $ \\alpha $ are the Lagrange Multipliers. The Lagrangian has to be minimised with to w,b and maximised with respect to $ \\alpha \\geq $ 0. \n",
    "\n",
    "The minimum with respect to w and b of the Lagrangian, $ \\phi $, is given by, \n",
    "$$  \\frac{\\partial \\phi}{\\partial b}=0  \\Longrightarrow \\sum_i^l \\alpha_{i} y_{i}=0 $$  \n",
    "\n",
    "$$  \\frac{\\partial \\phi}{\\partial w}=0 \\Longrightarrow w=\\sum_i^l \\alpha_{i} y_{i} x_{i} $$  \n",
    "\n",
    "When we rewrite the minimum Lagrangian by putting w and $$b=\\sum_i^l \\alpha_{i} y_{i}=0 $$ then we have the maximum Lagrangian $$ maxL=\\sum_i^l\\alpha_{i}-\\frac{1}{2}\\sum_i^l \\alpha_{i} \\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) $$ \n",
    "\n",
    "where $ \\alpha_{i} $'s are support vectors and positive. \n",
    " \n",
    "> The simplest way to separate two groups of data is with a straight line (1 dimension), flat plane (2 dimensions) or an N-dimensional hyperplane. However, there are situations where a nonlinear region can separate the groups more efficiently. SVM handles this by using a kernel function (nonlinear) to map the data into a different space where a hyperplane (linear) cannot be used to do the separation. It means a non-linear function is learned by a linear learning machine in a high-dimensional feature space while the capacity of the system is controlled by a parameter that does not depend on the dimensionality of the space. This is called kernel trick which means the kernel function transform the data into a higher dimensional feature space to make it possible to perform the linear separation. [3]\n",
    " \n",
    "$$ SVM =\\begin{cases}Linear SVM & x_{i}.x_{j}\\\\ Non-linear SVM & \\phi(x_{i}).\\phi(x_{j})\\\\Kernel Function & k(x_{i}.x_{j})\n",
    " \\end{cases}  $$\n",
    "\n",
    "\n",
    "The beauty of SVM is that if the data is linearly separable, there is a unique global minimum value. An ideal SVM analysis should produce a hyperplane that completely separates the vectors (cases) into two non-overlapping classes. However, perfect separation may not be possible, or it may result in a model with so many cases that the model does not classify correctly. In this situation SVM finds the hyperplane that maximizes the margin and minimizes the misclassifications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APPLY THE SUPPORT VECTOR MACHINE METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train_scaled = sc.fit_transform(X_train)\n",
    "X_test_scaled = sc.transform(X_test)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'linear')\n",
    "classifier.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine model is formed by using *classifier()*  from the library *sklearn* and then we construct the model with *fit()* code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[726  22]\n",
      " [ 39  37]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = classifier.predict(X_test_scaled)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix: \")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the first row , **748 customers** do not want to buy deposits (No answer)\n",
    "\n",
    "- We correctly predicted that *726 of them do not want to buy the deposit so customers gave answer \"No\"* . \n",
    "- We incorrectly predicted that *22 of them want to buy the deposit, customers gave answer \"Yes*. But in real customers gave answer \"No\". \n",
    "\n",
    "From the second row , **76 customers** want to buy deposits (Yes answer)\n",
    "\n",
    "- We incorrectly predicted that *39 of them do not want to buy the deposit, customers gave answer \"No\"* . But in real customers gave answer \"Yes\".\n",
    "- We correctly predicted that *37 of them want to buy the deposit so customers gave answer \"Yes*. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.97      0.96       748\n",
      "          1       0.63      0.49      0.55        76\n",
      "\n",
      "avg / total       0.92      0.93      0.92       824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Precision, Recall and F1-Score values are too high for answer \"Yes\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine Score= 0.925970873786\n"
     ]
    }
   ],
   "source": [
    "print(\"Support Vector Machine Score=\",classifier.score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The support vector machine score is consistent the classification report. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMPARING THE MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***Chi-Square Test to Logistic Regression Model:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[728  20]\n",
      " [ 42  34]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, predictions)\n",
    "print(\"Confusion Matrix: \")\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Power_divergenceResult(statistic=array([ 611.16363636,    3.62962963]), pvalue=array([  6.24677959e-135,   5.67594464e-002]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import chisquare\n",
    "obs=np.array([[728,20],[42,34]])\n",
    "chisquare(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***Chi-Square Test to Support Vector Machine Model:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[726  22]\n",
      " [ 39  37]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = classifier.predict(X_test_scaled)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix: \")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Power_divergenceResult(statistic=array([ 616.95294118,    3.81355932]), pvalue=array([  3.43941011e-136,   5.08393081e-002]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import chisquare\n",
    "obs=np.array([[726,22],[39,37]])\n",
    "chisquare(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Support Vector Machine Model has bigger chi-squared value so it is statistically good than Logistic Regression Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GROUP THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation of Columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>...</th>\n",
       "      <th>month_8</th>\n",
       "      <th>month_9</th>\n",
       "      <th>day_of_week_0</th>\n",
       "      <th>day_of_week_1</th>\n",
       "      <th>day_of_week_2</th>\n",
       "      <th>day_of_week_3</th>\n",
       "      <th>day_of_week_4</th>\n",
       "      <th>poutcome_0</th>\n",
       "      <th>poutcome_1</th>\n",
       "      <th>poutcome_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.028246</td>\n",
       "      <td>-0.008091</td>\n",
       "      <td>-0.014204</td>\n",
       "      <td>0.007179</td>\n",
       "      <td>0.013072</td>\n",
       "      <td>0.015529</td>\n",
       "      <td>0.086464</td>\n",
       "      <td>0.029247</td>\n",
       "      <td>0.009961</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023714</td>\n",
       "      <td>0.015196</td>\n",
       "      <td>0.002812</td>\n",
       "      <td>0.024298</td>\n",
       "      <td>-0.008800</td>\n",
       "      <td>0.044226</td>\n",
       "      <td>-0.063853</td>\n",
       "      <td>-0.002842</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>0.017587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>duration</th>\n",
       "      <td>0.028246</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.098310</td>\n",
       "      <td>-0.080311</td>\n",
       "      <td>0.056920</td>\n",
       "      <td>-0.068944</td>\n",
       "      <td>0.023705</td>\n",
       "      <td>-0.029160</td>\n",
       "      <td>-0.069410</td>\n",
       "      <td>-0.095347</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005613</td>\n",
       "      <td>0.024079</td>\n",
       "      <td>-0.015588</td>\n",
       "      <td>-0.021946</td>\n",
       "      <td>0.012390</td>\n",
       "      <td>0.017931</td>\n",
       "      <td>0.006862</td>\n",
       "      <td>0.015180</td>\n",
       "      <td>-0.055316</td>\n",
       "      <td>0.080605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>campaign</th>\n",
       "      <td>-0.008091</td>\n",
       "      <td>-0.098310</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.059960</td>\n",
       "      <td>-0.097753</td>\n",
       "      <td>0.173279</td>\n",
       "      <td>0.128599</td>\n",
       "      <td>0.014350</td>\n",
       "      <td>0.128145</td>\n",
       "      <td>0.153976</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.066038</td>\n",
       "      <td>-0.044614</td>\n",
       "      <td>0.040080</td>\n",
       "      <td>0.056928</td>\n",
       "      <td>-0.046224</td>\n",
       "      <td>-0.008625</td>\n",
       "      <td>-0.041640</td>\n",
       "      <td>-0.076579</td>\n",
       "      <td>0.096873</td>\n",
       "      <td>-0.055344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pdays</th>\n",
       "      <td>-0.014204</td>\n",
       "      <td>-0.080311</td>\n",
       "      <td>0.059960</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.507412</td>\n",
       "      <td>0.227693</td>\n",
       "      <td>0.013684</td>\n",
       "      <td>-0.083513</td>\n",
       "      <td>0.233311</td>\n",
       "      <td>0.296307</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.070393</td>\n",
       "      <td>-0.188869</td>\n",
       "      <td>0.003055</td>\n",
       "      <td>-0.011833</td>\n",
       "      <td>-0.001807</td>\n",
       "      <td>-0.016964</td>\n",
       "      <td>0.028334</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.488644</td>\n",
       "      <td>-0.942457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>previous</th>\n",
       "      <td>0.007179</td>\n",
       "      <td>0.056920</td>\n",
       "      <td>-0.097753</td>\n",
       "      <td>-0.507412</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.445049</td>\n",
       "      <td>-0.233331</td>\n",
       "      <td>-0.115118</td>\n",
       "      <td>-0.389316</td>\n",
       "      <td>-0.452448</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096377</td>\n",
       "      <td>0.153216</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>0.000771</td>\n",
       "      <td>0.001397</td>\n",
       "      <td>-0.001433</td>\n",
       "      <td>-0.012657</td>\n",
       "      <td>0.846434</td>\n",
       "      <td>-0.998001</td>\n",
       "      <td>0.471469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emp.var.rate</th>\n",
       "      <td>0.013072</td>\n",
       "      <td>-0.068944</td>\n",
       "      <td>0.173279</td>\n",
       "      <td>0.227693</td>\n",
       "      <td>-0.445049</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.575597</td>\n",
       "      <td>0.228463</td>\n",
       "      <td>0.796372</td>\n",
       "      <td>0.940285</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.173401</td>\n",
       "      <td>-0.137221</td>\n",
       "      <td>-0.018711</td>\n",
       "      <td>-0.019572</td>\n",
       "      <td>0.011872</td>\n",
       "      <td>0.018649</td>\n",
       "      <td>0.007305</td>\n",
       "      <td>-0.375434</td>\n",
       "      <td>0.446153</td>\n",
       "      <td>-0.215852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cons.price.idx</th>\n",
       "      <td>0.015529</td>\n",
       "      <td>0.023705</td>\n",
       "      <td>0.128599</td>\n",
       "      <td>0.013684</td>\n",
       "      <td>-0.233331</td>\n",
       "      <td>0.575597</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200624</td>\n",
       "      <td>0.355936</td>\n",
       "      <td>0.313130</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042026</td>\n",
       "      <td>0.033672</td>\n",
       "      <td>0.025207</td>\n",
       "      <td>0.012180</td>\n",
       "      <td>-0.036331</td>\n",
       "      <td>0.008961</td>\n",
       "      <td>-0.009130</td>\n",
       "      <td>-0.262834</td>\n",
       "      <td>0.244207</td>\n",
       "      <td>-0.019733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <td>0.086464</td>\n",
       "      <td>-0.029160</td>\n",
       "      <td>0.014350</td>\n",
       "      <td>-0.083513</td>\n",
       "      <td>-0.115118</td>\n",
       "      <td>0.228463</td>\n",
       "      <td>0.200624</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.237896</td>\n",
       "      <td>0.134503</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080015</td>\n",
       "      <td>0.116128</td>\n",
       "      <td>-0.001795</td>\n",
       "      <td>-0.039329</td>\n",
       "      <td>-0.024647</td>\n",
       "      <td>0.066625</td>\n",
       "      <td>-0.000479</td>\n",
       "      <td>-0.183737</td>\n",
       "      <td>0.121482</td>\n",
       "      <td>0.081139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>euribor3m</th>\n",
       "      <td>0.029247</td>\n",
       "      <td>-0.069410</td>\n",
       "      <td>0.128145</td>\n",
       "      <td>0.233311</td>\n",
       "      <td>-0.389316</td>\n",
       "      <td>0.796372</td>\n",
       "      <td>0.355936</td>\n",
       "      <td>0.237896</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.778794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.141886</td>\n",
       "      <td>-0.147038</td>\n",
       "      <td>0.046998</td>\n",
       "      <td>-0.132488</td>\n",
       "      <td>-0.013106</td>\n",
       "      <td>0.085449</td>\n",
       "      <td>0.015992</td>\n",
       "      <td>-0.307005</td>\n",
       "      <td>0.387323</td>\n",
       "      <td>-0.219871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nr.employed</th>\n",
       "      <td>0.009961</td>\n",
       "      <td>-0.095347</td>\n",
       "      <td>0.153976</td>\n",
       "      <td>0.296307</td>\n",
       "      <td>-0.452448</td>\n",
       "      <td>0.940285</td>\n",
       "      <td>0.313130</td>\n",
       "      <td>0.134503</td>\n",
       "      <td>0.778794</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.191010</td>\n",
       "      <td>-0.211561</td>\n",
       "      <td>-0.020874</td>\n",
       "      <td>-0.024064</td>\n",
       "      <td>0.025956</td>\n",
       "      <td>0.008917</td>\n",
       "      <td>0.009490</td>\n",
       "      <td>-0.341643</td>\n",
       "      <td>0.447520</td>\n",
       "      <td>-0.276489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     age  duration  campaign     pdays  previous  \\\n",
       "age             1.000000  0.028246 -0.008091 -0.014204  0.007179   \n",
       "duration        0.028246  1.000000 -0.098310 -0.080311  0.056920   \n",
       "campaign       -0.008091 -0.098310  1.000000  0.059960 -0.097753   \n",
       "pdays          -0.014204 -0.080311  0.059960  1.000000 -0.507412   \n",
       "previous        0.007179  0.056920 -0.097753 -0.507412  1.000000   \n",
       "emp.var.rate    0.013072 -0.068944  0.173279  0.227693 -0.445049   \n",
       "cons.price.idx  0.015529  0.023705  0.128599  0.013684 -0.233331   \n",
       "cons.conf.idx   0.086464 -0.029160  0.014350 -0.083513 -0.115118   \n",
       "euribor3m       0.029247 -0.069410  0.128145  0.233311 -0.389316   \n",
       "nr.employed     0.009961 -0.095347  0.153976  0.296307 -0.452448   \n",
       "\n",
       "                emp.var.rate  cons.price.idx  cons.conf.idx  euribor3m  \\\n",
       "age                 0.013072        0.015529       0.086464   0.029247   \n",
       "duration           -0.068944        0.023705      -0.029160  -0.069410   \n",
       "campaign            0.173279        0.128599       0.014350   0.128145   \n",
       "pdays               0.227693        0.013684      -0.083513   0.233311   \n",
       "previous           -0.445049       -0.233331      -0.115118  -0.389316   \n",
       "emp.var.rate        1.000000        0.575597       0.228463   0.796372   \n",
       "cons.price.idx      0.575597        1.000000       0.200624   0.355936   \n",
       "cons.conf.idx       0.228463        0.200624       1.000000   0.237896   \n",
       "euribor3m           0.796372        0.355936       0.237896   1.000000   \n",
       "nr.employed         0.940285        0.313130       0.134503   0.778794   \n",
       "\n",
       "                nr.employed     ...       month_8   month_9  day_of_week_0  \\\n",
       "age                0.009961     ...      0.023714  0.015196       0.002812   \n",
       "duration          -0.095347     ...     -0.005613  0.024079      -0.015588   \n",
       "campaign           0.153976     ...     -0.066038 -0.044614       0.040080   \n",
       "pdays              0.296307     ...     -0.070393 -0.188869       0.003055   \n",
       "previous          -0.452448     ...      0.096377  0.153216       0.012048   \n",
       "emp.var.rate       0.940285     ...     -0.173401 -0.137221      -0.018711   \n",
       "cons.price.idx     0.313130     ...     -0.042026  0.033672       0.025207   \n",
       "cons.conf.idx      0.134503     ...      0.080015  0.116128      -0.001795   \n",
       "euribor3m          0.778794     ...     -0.141886 -0.147038       0.046998   \n",
       "nr.employed        1.000000     ...     -0.191010 -0.211561      -0.020874   \n",
       "\n",
       "                day_of_week_1  day_of_week_2  day_of_week_3  day_of_week_4  \\\n",
       "age                  0.024298      -0.008800       0.044226      -0.063853   \n",
       "duration            -0.021946       0.012390       0.017931       0.006862   \n",
       "campaign             0.056928      -0.046224      -0.008625      -0.041640   \n",
       "pdays               -0.011833      -0.001807      -0.016964       0.028334   \n",
       "previous             0.000771       0.001397      -0.001433      -0.012657   \n",
       "emp.var.rate        -0.019572       0.011872       0.018649       0.007305   \n",
       "cons.price.idx       0.012180      -0.036331       0.008961      -0.009130   \n",
       "cons.conf.idx       -0.039329      -0.024647       0.066625      -0.000479   \n",
       "euribor3m           -0.132488      -0.013106       0.085449       0.015992   \n",
       "nr.employed         -0.024064       0.025956       0.008917       0.009490   \n",
       "\n",
       "                poutcome_0  poutcome_1  poutcome_2  \n",
       "age              -0.002842   -0.006591    0.017587  \n",
       "duration          0.015180   -0.055316    0.080605  \n",
       "campaign         -0.076579    0.096873   -0.055344  \n",
       "pdays             0.000140    0.488644   -0.942457  \n",
       "previous          0.846434   -0.998001    0.471469  \n",
       "emp.var.rate     -0.375434    0.446153   -0.215852  \n",
       "cons.price.idx   -0.262834    0.244207   -0.019733  \n",
       "cons.conf.idx    -0.183737    0.121482    0.081139  \n",
       "euribor3m        -0.307005    0.387323   -0.219871  \n",
       "nr.employed      -0.341643    0.447520   -0.276489  \n",
       "\n",
       "[10 rows x 39 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Correlation of Columns\")\n",
    "dataset.corr(method='spearman').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table include correlation numbers of first ten columns. We look at the table and delete some columns which have close correlation coefficients. \n",
    "\n",
    "For example, the variables *emp.var.rate, euribor3m  and nr.employed* have close  correlation coefficient, we use the strongest of them (nr.employed) to make the method more effective. And we simplify the data by making the following groupings on the data.\n",
    "\n",
    "\"divorced” and “single”  grouped together and call them “single”.\n",
    "\n",
    "“basic.4y”, “basic.9y” and “basic.6y” grouped together and call them “basic”.\n",
    "\n",
    "“admin.”, “management” and “entrepreneur” grouped together and call them 'high'; \"blue-collar\", \"technician\", \"services\" and \"housemaid\" grouped together and call them 'basic'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78643631144366166"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['emp.var.rate'].corr(dataset['euribor3m'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75331918713177837"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['nr.employed'].corr(dataset['euribor3m'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset2=pd.read_csv('Data-main.csv',header=0)\n",
    "dataset2=dataset2.dropna()\n",
    "dataset2.drop(\"duration\", axis=1, inplace=True)\n",
    "dataset2['education']=np.where(dataset2['education'] =='basic.9y', 'Basic', dataset2['education'])\n",
    "dataset2['education']=np.where(dataset2['education'] =='basic.6y', 'Basic', dataset2['education'])\n",
    "dataset2['education']=np.where(dataset2['education'] =='basic.4y', 'Basic', dataset2['education'])\n",
    "\n",
    "dataset2['job']=np.where(dataset2['job'] =='admin.', 'High', dataset2['job'])\n",
    "dataset2['job']=np.where(dataset2['job'] =='management', 'High', dataset2['job'])\n",
    "dataset2['job']=np.where(dataset2['job'] =='blue-collar', 'Basic', dataset2['job'])\n",
    "dataset2['job']=np.where(dataset2['job'] =='services', 'Basic', dataset2['job'])\n",
    "dataset2['job']=np.where(dataset2['job'] =='housemaid', 'Basic', dataset2['job'])\n",
    "dataset2['job']=np.where(dataset2['job'] =='technician', 'Basic', dataset2['job'])\n",
    "\n",
    "categorical_vars = dataset2.describe(include=[\"object\"]).columns\n",
    "data_dummies = pd.get_dummies(dataset2, columns=categorical_vars, drop_first=True)\n",
    "\n",
    "dataset2['default'] = dataset2['default'].astype('category')\n",
    "dataset2['housing'] = dataset2['housing'].astype('category')\n",
    "dataset2['loan'] = dataset2['loan'].astype('category')\n",
    "dataset2['contact'] = dataset2['contact'].astype('category')\n",
    "dataset2['month'] = dataset2['month'].astype('category')\n",
    "dataset2['day_of_week'] = dataset2['day_of_week'].astype('category')\n",
    "dataset2['poutcome'] = dataset2['poutcome'].astype('category')\n",
    "\n",
    "categoricals = list(dataset2.dtypes[dataset2.dtypes == 'category'].index)\n",
    "columnnames = list(dataset2)\n",
    "\n",
    "datasetNumerics = dataset2.drop(categoricals,1)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "classes = []\n",
    "for i in categoricals:\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    dataset2[i] = le.fit_transform(dataset2[i].as_matrix())\n",
    "    classes.append(list(le.classes_))\n",
    "    \n",
    "enc=preprocessing.OneHotEncoder(dtype=np.int32,sparse=False)\n",
    "X = enc.fit_transform(dataset2[categoricals])\n",
    "unique = dataset2[categoricals].apply(lambda x: x.value_counts()).unstack()\n",
    "unique = unique[~unique.isnull()]\n",
    "dataset2[categoricals].head()\n",
    "enc_cols = list(unique.index.map('{0[0]}_{0[1]}'.format)) \n",
    "\n",
    "datasetCategoricals = pd.DataFrame(X, columns=enc_cols, index=dataset2[categoricals].index)\n",
    "dataset2 = datasetNumerics.merge(datasetCategoricals,left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train and test values: (3295, 29) (824, 29) (3295,) (824,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2 , random_state=25)\n",
    "print(\"train and test values:\",X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***Again apply the Logistic Regression Method:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "model=logreg.fit(X_train, y_train)\n",
    "predictions = logreg.predict(X_test)\n",
    "print (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[735  13]\n",
      " [ 63  13]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, predictions)\n",
    "print(\"Confusion Matrix: \")\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the first row , **748 customers** do not want to buy deposits (No answer)\n",
    "\n",
    "- We correctly predicted that *735 of them do not want to buy the deposit so customers gave answer \"No\"* . \n",
    "- We incorrectly predicted that *13 of them want to buy the deposit, customers gave answer \"Yes*. But in real customers gave answer \"No\". \n",
    "\n",
    "From the second row , **76 customers** want to buy deposits (Yes answer)\n",
    "\n",
    "- We incorrectly predicted that *63 of them do not want to buy the deposit, customers gave answer \"No\"* . But in real customers gave answer \"Yes\".\n",
    "- We correctly predicted that *13 of them want to buy the deposit so customers gave answer \"Yes*. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.98      0.95       748\n",
      "          1       0.50      0.17      0.25        76\n",
      "\n",
      "avg / total       0.88      0.91      0.89       824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be easily seen that the value of the 2nd line in the table falls. More accurate results were obtained when data with close correlation coefficient were removed. This means that post-grouping method work better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***Again apply the Support Vector Machine Method:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = StandardScaler()\n",
    "X_train_scaled = sc.fit_transform(X_train)\n",
    "X_test_scaled = sc.transform(X_test)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'linear')\n",
    "classifier.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[733  15]\n",
      " [ 61  15]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test_scaled)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix: \")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the first row , **748 customers** do not want to buy deposits (No answer)\n",
    "\n",
    "- We correctly predicted that *733 of them do not want to buy the deposit so customers gave answer \"No\"* . \n",
    "- We incorrectly predicted that *15 of them want to buy the deposit, customers gave answer \"Yes*. But in real customers gave answer \"No\". \n",
    "\n",
    "From the second row , **76 customers** want to buy deposits (Yes answer)\n",
    "\n",
    "- We incorrectly predicted that *61 of them do not want to buy the deposit, customers gave answer \"No\"* . But in real customers gave answer \"Yes\".\n",
    "- We correctly predicted that *15 of them want to buy the deposit so customers gave answer \"Yes*. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.98      0.95       748\n",
      "          1       0.50      0.20      0.28        76\n",
      "\n",
      "avg / total       0.88      0.91      0.89       824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again it can be easily seen that the value of the 2nd line in the table falls. More accurate results were obtained when data with close correlation coefficient were removed. This means that post-grouping method work better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMPARE THE MODELS AFTER GROUPING THE DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***Chi-Square Test to Logistic Regression Model:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[735  13]\n",
      " [ 63  13]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, predictions)\n",
    "print(\"Confusion Matrix: \")\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Power_divergenceResult(statistic=array([ 565.89473684,    0.        ]), pvalue=array([  4.38861389e-125,   1.00000000e+000]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import chisquare\n",
    "obs=np.array([[735,13],[63,13]])\n",
    "chisquare(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***Chi-Square Test to Support Vector Machine Model:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[733  15]\n",
      " [ 61  15]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test_scaled)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix: \")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Power_divergenceResult(statistic=array([ 616.95294118,    3.81355932]), pvalue=array([  3.43941011e-136,   5.08393081e-002]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import chisquare\n",
    "obs=np.array([[726,22],[39,37]])\n",
    "chisquare(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Support Vector Machine Model has bigger chi-squared value and it has smaller p value. Then we can say again that it is statistically good than Logistic Regression Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THE RESULT\n",
    "\n",
    "According top the results these codes, if we compare of two model that we constituted of the Support Vector Machine model is better than the Logistic Regression Model because the chi-squared values of Support vector Machine model is greater than Logistic Regression model, and also Support Vector Machine Model is easier to form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REFERENCES\n",
    "\n",
    "\n",
    "[1] http://archive.ics.uci.edu/ml/datasets/Bank+Marketing\n",
    "\n",
    "[2] http://www.saedsayad.com/logistic_regression.htm \n",
    "\n",
    "[3] http://www.saedsayad.com/images/SVM_2.png\n",
    "\n",
    "[4] https://s3.amazonaws.com/quantstart/media/images/qs-svm-0003.png \n",
    "\n",
    "[5] David W. Hosmer, Stanley Lemeshow ,*Logistic Regression*, http://resource.heartonline.cn/20150528/1_3kOQSTg.pdf \n",
    "\n",
    "[6] Penn Stage Eberly College of Science, https://onlinecourses.science.psu.edu/stat504/node/172 [online]\n",
    "\n",
    "[7] Hiroyasu Yamada, Yuji Matsumoto *STATISTICAL DEPENDENCY ANALYSIS WITH SUPPORT VECTOR MACHINES*,http://www.jaist.jp/~h-yamada/pdf/iwpt2003.pdf [online]\n",
    "\n",
    "[8] Gunn, Steve R. *Support Vector Machines for Classification and Regression*,10 May 1998\n",
    "\n",
    "[9] http://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_multivariable/BS704_Multivariable8.html [online]\n",
    "\n",
    "[10] Penn Stage Eberly College of Science,\n",
    "http://www.statsoft.com/Textbook/Support-Vector-Machines#overview [online]\n",
    "\n",
    "[11] http://www.saedsayad.com/support_vector_machine.htm [online]\n",
    "\n",
    "[12] http://www.statsoft.com/Textbook/Support-Vector-Machines#overview [online]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
