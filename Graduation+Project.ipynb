{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMPARISON OF LOGISTIC REGRESSION AND SUPPORT VECTOR MACHÄ°NE \n",
    "\n",
    "\n",
    "### Data Set Information\n",
    "\n",
    "- The data is related with direct marketing campaigns of a Portuguese banking institution [1]. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed. \n",
    "\n",
    "- The data has 86520 entries. Each column contains 4119 entry and there are 21 columns which contain client data, contact information, attributes and output variable.\n",
    "\n",
    "\n",
    "#### Client Data\n",
    "\n",
    "**1. Age:** This column indicates the age of the customer. It is a numeric variable and ranges between 18 and 88. \t\n",
    "\n",
    "**2. Job:** This column indicates the job of the customer. There are only a few options. These are\n",
    "- 'admin',\n",
    "- 'blue-collar',\n",
    "- 'entrepreneur',\n",
    "- 'housemaid',\n",
    "- 'management',\n",
    "- 'retired',\n",
    "- 'self-employed',\n",
    "- 'services',\n",
    "- 'student',\n",
    "- 'technician',\n",
    "- 'unemployed',\n",
    "- 'unknown'\n",
    "\n",
    "**3. Marital:** This column indicates the marital status of the customer. It is a categorical variable and the options are\n",
    "\n",
    "- 'divorced',\n",
    "- 'married',\n",
    "- 'single',\n",
    "- 'unknown'\n",
    "\n",
    "**4.Education:** This column indicates the education status of the customer. It is a categorical variable and the options are \n",
    "- 'basic.4y',\n",
    "- 'basic.6y',\n",
    "- 'basic.9y',\n",
    "- 'high.school',\n",
    "- 'illiterate',\n",
    "- 'professional.course',\n",
    "- 'university.degree',\n",
    "- 'unknown'\n",
    "\n",
    "**5.Default:** This column indicates that customers have credit in default. It is a categorical variable and there are three options. These are \n",
    "\n",
    "- 'no',\n",
    "- 'yes',\n",
    "- 'unknown'\n",
    "\n",
    "**6.Housing:** This column indicates that customers have housing loan. It is a categorical variable and there are three options. These are \n",
    "\n",
    "- 'no',\n",
    "- 'yes',\n",
    "- 'unknown'\n",
    "\n",
    "**7.Loan:** This column indicates that customers have personal loan. It is a categorical variable and there are three options. These are  \n",
    "\n",
    "- 'no',\n",
    "- 'yes',\n",
    "- 'unknown'\n",
    "\n",
    "\n",
    "#### Information Of The Last Contact\n",
    "\n",
    "**8.Contact:** This column indicates the type of communication established with the customer. It is a categorical variable and there are two options. These are \n",
    "\n",
    "- 'cellular',\n",
    "- 'telephone'\n",
    "\n",
    "**9.Month:** This column indicates that month of year which is last contact with the customer. It is a categorical variable and the options are\n",
    "\n",
    "- 'jan', \n",
    "- 'feb',\n",
    "- 'mar',\n",
    "- 'apr',\n",
    "- 'may',\n",
    "- 'jun',\n",
    "- 'jul',\n",
    "- 'aug',\n",
    "- 'sep',\n",
    "- 'oct',\n",
    "- 'nov',\n",
    "- 'dec'\n",
    "\n",
    "**10.Day_of_week:** This column shows that day of week which is last contact with the customer. It is a categorical variable and the options are\n",
    "\n",
    "- 'mon',\n",
    "- 'tue',\n",
    "- 'wed',\n",
    "- 'thu',\n",
    "- 'fri'\n",
    "\n",
    "**11.Duration:** This column indicates that how long the last duration of contact with the customer. It is a numeric variable and ranges between 0 and 3643 seconds. If the duration is 0 ,it means that the output variable is 'no'. Also the duration is not known before a call is performed. After the end of the call, y is obviously known. Thus, we  should only include this input for benchmark purposes.\n",
    "\n",
    "\n",
    "#### Other Attributes\n",
    "\n",
    "**12.Campaign:** This column indicates that number of contacts performed during this campaign and for this client. It is a numeric variable and ranges between 1 and 35.\t\n",
    "\n",
    "**13.Pdays:** This column indicates that number of days that passed by after the client was last contacted from a previous campaign. It is a numeric variable and ranges between 0 and 999. Also 999 means that client was not previously contacted.\t\n",
    "\n",
    "**14.Previous:** This column indicates that number of contacts performed before this campaign and for this client. It is a numeric variable and ranges between 0 and 6.\n",
    "\n",
    "**15.Poutcome:** This column indicates that outcome of the previous marketing campaign. It is a categorical variable and the options are   \n",
    "\n",
    "- 'failure',\n",
    "- 'nonexistent',\n",
    "- 'success'\n",
    "\n",
    "\n",
    "#### Social And Economic Context Attributes \n",
    "\n",
    "**16.Emp.var.rate:** This column indicates that employment variation rate. It is a numeric variable and ranges between -3 and 1.4. In addition this rate came from quarterly indicator.\t\n",
    "\n",
    "**17.Cons.price.idx:** This column indicates that consumer price index. It is a numeric variable and ranges between 92201 and 94767. In addition this index come from monthly indicator.\n",
    "\n",
    "**18.Cons.conf.idx:** This column indicates that consumer confidence index. It is a numeric variable and ranges between -50 and -33. In addition this index come from monthly indicator.\t\n",
    "\n",
    "**19.Euribor3m:** This column indicates that Euribor 3 month rate which come daily indicator. It is a numeric variable and ranges between 1 and 5045.\n",
    "\n",
    "**20.Nr.employed:** This column indicates that number of employees which come quarterly indicator. It is also a numeric variable and ranges between 5191 and 5228.1. \n",
    "\n",
    "#### Output Variable \n",
    "\n",
    "**21.Y:** This column indicates that has the client subscribed a term deposit. It is a binary variable and the options are \n",
    "\n",
    "- 'yes',\n",
    "- 'no'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Overview of the data:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Data = (4119, 21)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>...</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45</td>\n",
       "      <td>entrepreneur</td>\n",
       "      <td>married</td>\n",
       "      <td>university.degree</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>cellular</td>\n",
       "      <td>aug</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.4</td>\n",
       "      <td>93444.0</td>\n",
       "      <td>-36.1</td>\n",
       "      <td>4965.0</td>\n",
       "      <td>5228.1</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "      <td>technician</td>\n",
       "      <td>single</td>\n",
       "      <td>university.degree</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>cellular</td>\n",
       "      <td>aug</td>\n",
       "      <td>wed</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.4</td>\n",
       "      <td>93444.0</td>\n",
       "      <td>-36.1</td>\n",
       "      <td>4967.0</td>\n",
       "      <td>5228.1</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>cellular</td>\n",
       "      <td>aug</td>\n",
       "      <td>wed</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.4</td>\n",
       "      <td>93444.0</td>\n",
       "      <td>-36.1</td>\n",
       "      <td>4965.0</td>\n",
       "      <td>5228.1</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38</td>\n",
       "      <td>technician</td>\n",
       "      <td>married</td>\n",
       "      <td>professional.course</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>aug</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.4</td>\n",
       "      <td>93444.0</td>\n",
       "      <td>-36.1</td>\n",
       "      <td>4965.0</td>\n",
       "      <td>5228.1</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>university.degree</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>aug</td>\n",
       "      <td>tue</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.4</td>\n",
       "      <td>93444.0</td>\n",
       "      <td>-36.1</td>\n",
       "      <td>4963.0</td>\n",
       "      <td>5228.1</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age           job  marital            education  default housing loan  \\\n",
       "0   45  entrepreneur  married    university.degree  unknown     yes  yes   \n",
       "1   29    technician   single    university.degree       no     yes  yes   \n",
       "2   40    management  married          high.school       no      no  yes   \n",
       "3   38    technician  married  professional.course       no     yes   no   \n",
       "4   34        admin.  married    university.degree       no      no   no   \n",
       "\n",
       "    contact month day_of_week ...   campaign  pdays  previous     poutcome  \\\n",
       "0  cellular   aug         mon ...          2    999         0  nonexistent   \n",
       "1  cellular   aug         wed ...          3    999         0  nonexistent   \n",
       "2  cellular   aug         wed ...          1    999         0  nonexistent   \n",
       "3  cellular   aug         mon ...          1    999         0  nonexistent   \n",
       "4  cellular   aug         tue ...          1    999         0  nonexistent   \n",
       "\n",
       "  emp.var.rate  cons.price.idx  cons.conf.idx  euribor3m  nr.employed    y  \n",
       "0          1.4         93444.0          -36.1     4965.0       5228.1   no  \n",
       "1          1.4         93444.0          -36.1     4967.0       5228.1   no  \n",
       "2          1.4         93444.0          -36.1     4965.0       5228.1   no  \n",
       "3          1.4         93444.0          -36.1     4965.0       5228.1  yes  \n",
       "4          1.4         93444.0          -36.1     4963.0       5228.1   no  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset=pd.read_csv('Data-main.csv',header=0)\n",
    "print(\"Size of Data =\", dataset.shape)\n",
    "dataset=dataset.dropna()\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially we construct data for *.head()* code and we see the first six columns of the data and we see the size of data with *.shape()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4119.000000</td>\n",
       "      <td>4119.000000</td>\n",
       "      <td>4119.000000</td>\n",
       "      <td>4119.000000</td>\n",
       "      <td>4119.000000</td>\n",
       "      <td>4119.000000</td>\n",
       "      <td>4119.000000</td>\n",
       "      <td>4119.000000</td>\n",
       "      <td>4119.000000</td>\n",
       "      <td>4119.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>40.113620</td>\n",
       "      <td>256.788055</td>\n",
       "      <td>2.537266</td>\n",
       "      <td>960.422190</td>\n",
       "      <td>0.190337</td>\n",
       "      <td>0.084972</td>\n",
       "      <td>84854.473707</td>\n",
       "      <td>-40.499102</td>\n",
       "      <td>3244.553921</td>\n",
       "      <td>5166.481695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10.313362</td>\n",
       "      <td>254.703736</td>\n",
       "      <td>2.568159</td>\n",
       "      <td>191.922786</td>\n",
       "      <td>0.541788</td>\n",
       "      <td>1.563114</td>\n",
       "      <td>27265.192237</td>\n",
       "      <td>4.594578</td>\n",
       "      <td>1995.780101</td>\n",
       "      <td>73.667904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.400000</td>\n",
       "      <td>93.200000</td>\n",
       "      <td>-50.800000</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>4963.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>32.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.800000</td>\n",
       "      <td>92893.000000</td>\n",
       "      <td>-42.700000</td>\n",
       "      <td>1281.000000</td>\n",
       "      <td>5099.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>38.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>93749.000000</td>\n",
       "      <td>-41.800000</td>\n",
       "      <td>4856.000000</td>\n",
       "      <td>5191.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>47.000000</td>\n",
       "      <td>317.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>93994.000000</td>\n",
       "      <td>-36.400000</td>\n",
       "      <td>4961.000000</td>\n",
       "      <td>5228.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.000000</td>\n",
       "      <td>3643.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>94767.000000</td>\n",
       "      <td>-26.900000</td>\n",
       "      <td>5045.000000</td>\n",
       "      <td>5228.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               age     duration     campaign        pdays     previous  \\\n",
       "count  4119.000000  4119.000000  4119.000000  4119.000000  4119.000000   \n",
       "mean     40.113620   256.788055     2.537266   960.422190     0.190337   \n",
       "std      10.313362   254.703736     2.568159   191.922786     0.541788   \n",
       "min      18.000000     0.000000     1.000000     0.000000     0.000000   \n",
       "25%      32.000000   103.000000     1.000000   999.000000     0.000000   \n",
       "50%      38.000000   181.000000     2.000000   999.000000     0.000000   \n",
       "75%      47.000000   317.000000     3.000000   999.000000     0.000000   \n",
       "max      88.000000  3643.000000    35.000000   999.000000     6.000000   \n",
       "\n",
       "       emp.var.rate  cons.price.idx  cons.conf.idx    euribor3m  nr.employed  \n",
       "count   4119.000000     4119.000000    4119.000000  4119.000000  4119.000000  \n",
       "mean       0.084972    84854.473707     -40.499102  3244.553921  5166.481695  \n",
       "std        1.563114    27265.192237       4.594578  1995.780101    73.667904  \n",
       "min       -3.400000       93.200000     -50.800000     0.640000  4963.600000  \n",
       "25%       -1.800000    92893.000000     -42.700000  1281.000000  5099.100000  \n",
       "50%        1.100000    93749.000000     -41.800000  4856.000000  5191.000000  \n",
       "75%        1.400000    93994.000000     -36.400000  4961.000000  5228.100000  \n",
       "max        1.400000    94767.000000     -26.900000  5045.000000  5228.100000  "
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we look at the describing information of numerical columns. It is obviously numerical column values are not close each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               0\n",
       "job               0\n",
       "marital           0\n",
       "education         0\n",
       "default           0\n",
       "housing           0\n",
       "loan              0\n",
       "contact           0\n",
       "month             0\n",
       "day_of_week       0\n",
       "duration          0\n",
       "campaign          0\n",
       "pdays             0\n",
       "previous          0\n",
       "poutcome          0\n",
       "emp.var.rate      0\n",
       "cons.price.idx    0\n",
       "cons.conf.idx     0\n",
       "euribor3m         0\n",
       "nr.employed       0\n",
       "y                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If any statistical method is applied, successful operation can not be obtained if there are deleted or damaged cells in the data.\n",
    "\n",
    "It seems that there is no undefined value or null cell in data. This means that we can efficiently complete the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEICAYAAACwDehOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUJAkNYaCJKkxFCRJzf8Hywhpb8UDYiYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113e38cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_name = ('Yes','No')\n",
    "dataset['y_new']=dataset.y.map({'yes':1, 'no':0})\n",
    "incomes = (0.1094926,0.8905074)\n",
    "y_post = np.arange(len(y_name))\n",
    "plt.bar( y_post,incomes,align='center',alpha=0.7)\n",
    "plt.xticks(y_post,y_name)\n",
    "plt.ylabel('Counts')\n",
    "plt.title('Independent Variable')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here y values are seen to be unbalanced. \n",
    "\n",
    "*451 of the customer*, which corresponds to approximately **11%**, gave a \"Yes\" answer when the customer was asked to sign up for a time deposit and *3668 of the customer*, which corresponds to approximately **89%**, gave \"No\" reply when the customer was asked to sign up for a time deposit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'duration', 'campaign', 'pdays', 'previous', 'emp.var.rate',\n",
       "       'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed', 'y_new',\n",
       "       'job_blue-collar', 'job_entrepreneur', 'job_housemaid',\n",
       "       'job_management', 'job_retired', 'job_self-employed', 'job_services',\n",
       "       'job_student', 'job_technician', 'job_unemployed', 'job_unknown',\n",
       "       'marital_married', 'marital_single', 'marital_unknown',\n",
       "       'education_basic.6y', 'education_basic.9y', 'education_high.school',\n",
       "       'education_illiterate', 'education_professional.course',\n",
       "       'education_university.degree', 'education_unknown', 'default_unknown',\n",
       "       'default_yes', 'housing_unknown', 'housing_yes', 'loan_unknown',\n",
       "       'loan_yes', 'contact_telephone', 'month_aug', 'month_dec', 'month_jul',\n",
       "       'month_jun', 'month_mar', 'month_may', 'month_nov', 'month_oct',\n",
       "       'month_sep', 'day_of_week_mon', 'day_of_week_thu', 'day_of_week_tue',\n",
       "       'day_of_week_wed', 'poutcome_nonexistent', 'poutcome_success', 'y_yes'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_vars = dataset.describe(include=[\"object\"]).columns\n",
    "data_dummies = pd.get_dummies(dataset, columns=categorical_vars, drop_first=True)\n",
    "data_dummies.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy Variables=\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, ..., 0, 1, 0],\n",
       "       [0, 1, 0, ..., 0, 1, 0],\n",
       "       [0, 1, 0, ..., 0, 1, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0]], dtype=int32)"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.drop(['marital','job','education','y'],1, inplace=True)\n",
    "dataset['default'] = dataset['default'].astype('category')\n",
    "dataset['housing'] = dataset['housing'].astype('category')\n",
    "dataset['loan'] = dataset['loan'].astype('category')\n",
    "dataset['contact'] = dataset['contact'].astype('category')\n",
    "dataset['month'] = dataset['month'].astype('category')\n",
    "dataset['day_of_week'] = dataset['day_of_week'].astype('category')\n",
    "dataset['poutcome'] = dataset['poutcome'].astype('category')\n",
    "\n",
    "categoricals = list(dataset.dtypes[dataset.dtypes == 'category'].index)\n",
    "columnnames = list(dataset)\n",
    "\n",
    "datasetNumerics = dataset.drop(categoricals,1)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "classes = []\n",
    "for i in categoricals:\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    dataset[i] = le.fit_transform(dataset[i].as_matrix())\n",
    "    classes.append(list(le.classes_))\n",
    "    \n",
    "enc=preprocessing.OneHotEncoder(dtype=np.int32,sparse=False)\n",
    "X = enc.fit_transform(dataset[categoricals])\n",
    "unique = dataset[categoricals].apply(lambda x: x.value_counts()).unstack()\n",
    "unique = unique[~unique.isnull()]\n",
    "dataset[categoricals].head()\n",
    "enc_cols = list(unique.index.map('{0[0]}_{0[1]}'.format))\n",
    "\n",
    "datasetCategoricals = pd.DataFrame(X, columns=enc_cols, index=dataset[categoricals].index)\n",
    "dataset = datasetNumerics.merge(datasetCategoricals,left_index = True, right_index = True)\n",
    "\n",
    "y = dataset['y_new']\n",
    "del dataset['y_new']\n",
    "X = dataset\n",
    "\n",
    "print(\"Dummy Variables=\")\n",
    "dataset.iloc[:,21:].values \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our independent variables are composed of many different categorical values. For this reason, we are creating dummy variables that we code as 0 and 1 in order to complete to apply the methods in the python. And categorical variable defined as category. Then using *Encoder and OneHotEncoder* dummy variables have been created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# LOGISTIC REGRESSION\n",
    "\n",
    "#### *Introduction to Logistic Regression Model*\n",
    "                                                                        \n",
    "*Logistic Regression* is a simple machine learning method for analyzing a dataset you can use to predict the value of a numeric categorical variable based on its relationship with predictor variables. Like all other statistical methods,the main purpose here is the best fitting and biologically reasonable model to describe the relationship. \n",
    "\n",
    "There is an *Outcome variable* which is called that *dependent* or *response* and also there are *independent variables* that are called that *predictor* or *explanatory*.\n",
    "\n",
    "In regression problems, \"*E (Y | x) *\" is called the conditional average and gives the average of the result variable. *Y denotes the outcome variable* and *x denotes a value of the independent variable. E(Y|x) is read *\"the expected value of Y, given the value x.\"*\n",
    "\n",
    "In the *linear regression*, we assume that this average can be expressed linearly (or some transformations of x or Y) in x such as ***$ E (Y | x) = \\beta_ {0} + \\beta_ {1}x+ \\epsilon_{i} $.*** \n",
    "\n",
    "$ \\epsilon _{i} $, is called the error term. This variable captures all other factors which influence the dependent variable $ y_{i} $ other than the regressors $ x_{i} $.  \n",
    "This implies that it is possible for E (Y | x) to take any value, such as x intervals between $-â$ and $+â$.\n",
    "\n",
    "   \n",
    "What distinguishes a logistic regression from the linear regression model is using several predictors which are numerical and categorical. A linear regression is not appropriate for predicting the value of a binary variable for two reasons:\n",
    "\n",
    "-  A linear regression will predict values outside the acceptable range 0 to 1.\n",
    "\n",
    "- The dichotomous experiments can only have one of two possible values for each experiment, therefore the residuals will not be normally distributed about the predicted line.\n",
    "\n",
    "![](http://www.saedsayad.com/images/LogReg_1.png)\n",
    "\n",
    "> On the other hand, a logistic regression produces a logistic curve, which is limited to values between 0 and 1. The curve is said to be S-shaped.\n",
    "In the logistic regression the constant (b0) moves the curve left and right and the slope (b1) defines the steepness of the curve. By simple transformation, the logistic regression equation can be written in terms of an odds ratio.\n",
    "\n",
    "> Finally, taking the natural log of both sides, we can write the equation in terms of log-odds (logit) which is a linear function of the predictors. The coefficient (b1) is the amount the logit (log-odds) changes with a one unit change in x.     *** [2] ***\n",
    "\n",
    "\n",
    "#### Interpretation of Fitting Logistic Regression Analysis\n",
    "\n",
    "*A link function* is simply a function of the mean of the response variable Y that we use as the response instead of Y itself. In this case, the linear regression model is an identification function because it is linear in the dependent variable parameters required by the definition.\n",
    "\n",
    "In order to simplify notation, when the logistic distribution is used we use the quantity $ \\pi(x)= E (Y | x) $.The specific form of the logistic regression model that:\n",
    "\n",
    "$$ \\pi(x)= \\frac{e^{\\beta_{0} +\\beta_{1}}}{1+e^{\\beta_{0} +\\beta_{1}}} $$\n",
    "\n",
    "Then there are three more important description that need to be mentioned,*\"odds\"*, *\"odds ratio\"* and *\"logit transformation\"*:\n",
    "\n",
    "- The **odds** of the dependent variable equaling a case (given some linear combination x of the predictors) is equivalent to the exponential function of the linear regression expression.\n",
    "\n",
    "$$ odds= e^{\\beta_{0} +\\beta_{1}x} $$\n",
    "\n",
    "- Then **odds ratio** occurs like that\n",
    "\n",
    "$$ OR=\\frac{odds(x+1)}{odds(x)}=\\frac{e^{\\beta_{0} +\\beta_{1}x}}{1+e^{\\beta_{0} +\\beta_{1}(x+1)}}= e^{\\beta_{1}}$$\n",
    "\n",
    "- Also in the logistic regression model the *link function* is the **logit transformation** which is \n",
    "\n",
    "$$ g(x)=\\ln{\\frac{\\pi(x)}{1-\\pi(x)}}=\\beta_{0} +\\beta_{1}x $$.\n",
    "\n",
    "\n",
    "In the regression model, the slope coefficient represents the change in the logit corresponding to a change of one unit in the independent variable (i.e. $\\beta_{1}= g(x+1)-g(x)$).\n",
    "\n",
    "To be able to interpret the coefficient properly in a logistic regression model depends on the correct meaning of the difference between the two logits. The interpretation of this distinction is examined in detail on a case-by-case basis, since it is directly dependent on the definition and meaning of an independent unit change.\n",
    "\n",
    "\n",
    "#### Dichotomous Independent Variable\n",
    "\n",
    "\n",
    "When interpreting the logistic regression coefficients, it should be taken into account that the independent variable is nominal scale and binary (ie measured at two levels). \n",
    "\n",
    "We assume that the argument is encoded as x or zero. The logit difference of a subject with X = 1 and x = 0 would be:\n",
    "\n",
    "$$ g (1)-g (0) = [\\beta_ {0} + \\beta_ {1} x] - \\beta_ {0} = \\beta_ {1}. $$\n",
    "\n",
    "\n",
    "The odds of the outcome being present among individuals with $x=1$ is defined as $\\frac{\\pi(1)}{1-\\pi(1)}$. Similarly the odds of the outcome being present among individuals with $x=0$ is defined as $\\frac{\\pi(0)}{1-\\pi(0)}$. The odds ratio is defined as the ratio of the odds for $x=1$ to the odds for $x=0$ and is given by the equation\n",
    "\n",
    "$$\n",
    "\\text{Odds Ratio} = \\frac{\\frac{\\pi(1)}{1-\\pi(1)}}{\\frac{\\pi(0)}{1-\\pi(0)}}\n",
    "$$\n",
    "\n",
    "Hence for logistic regression with a dichotomous independent variable coded 1 and 0, the relationship between the odds ratio and the regression coefficient is Odds Ratio$= \\mathrm{e}^{\\beta_{1}}$.\n",
    "This simple relationship between the coefficient and the odds ratio is the fundamental reason why logistic regression has proven to be such a powerful analytic research tool.\n",
    "\n",
    "The interpretation given for the odds ratio is based on the fact that in many instances it approximates a quantity called the relative risk. This parameter is equal to the ratio $\\frac{\\pi(1)}{\\pi(0)}$. It follows that the odds ratio approximates the relative risk $\\frac{1-\\pi(0)}{1-\\pi(1)}\\approx 1$. This holds when $\\pi(x)$ is small for both $x=1$ and $x=0$.\n",
    "\n",
    "\n",
    "#### Polytomous (Multinomial) Logistic Regression Analysis\n",
    "\n",
    "\n",
    "Where the response is a binary variable with 'success' and 'failure' being only two categories, logistic regression can be applied. But logistic regression can be extended to handle responses, Y, that are polytomous, i.e. taking $ r>2 $ categories.\n",
    "\n",
    "When $ r > 2 $, we have a multi-category or polytomous response variable. There are $r (r â 1)/2$ logits (odds) that we can form, but only $(r â 1)$ are non-redundant. There are different ways to form a set of $(r â 1)$ non-redundant logits, and these will lead to different polytomous (multinomial) logistic regression models.\n",
    "\n",
    "Multinomial Logistic Regression models how multinomial response variable Y depends on a set of k explanatory variables $x=(x_{1},x_{2},...,x_{p})$. This is also a *Generalized Linear Model* where the random component assumes that the distribution of Y is Multinomial(n,Ï), where Ï is a vector with probabilities of \"success\" for each category. The systematic component are explanatory variables (can be continuous, discrete, or both) and are linear in the parameters, e.g.,\n",
    "\n",
    "$$ \\beta_{0} +\\beta_{1}x_{1}+\\beta_{2}x_{2}+...+\\beta_{p}x_{p} $$. \n",
    "\n",
    "Again, transformation of the X's themselves are allowed like in linear regression. The link function is the generalized Logit, that it the logit link for each pair of non-redundant logits as discussed above.\n",
    "\n",
    "For the moment we will assume that each of these variables is at least interval scale. Let the conditional probability that the outcome is present be denoted by $P(Y=1|x)= \\pi(x)$. The logit of the multiple logistic regression model is given by the equation\n",
    "\n",
    "$$ g(x)=\\beta_{0} +\\beta_{1}x_{1}+\\beta_{2}x_{2}+...+\\beta_{p}x_{p} $$\n",
    "\n",
    "in which case the logistic regression model is\n",
    "\n",
    "$$ \\pi(x)=\\frac{\\mathrm{e}^{g(x)}}{1+\\mathrm{e}^{g(x)}} $$\n",
    "\n",
    "The multiple logistic regression model can be written as follows:\n",
    "\n",
    "$$\n",
    "\\hat{p} = \\frac{\\beta_{0} +\\beta_{1}x_{1}+\\beta_{2}x_{2}+...+\\beta_{p}x_{p}}{1-\\beta_{0} +\\beta_{1}x_{1}+\\beta_{2}x_{2}+...+\\beta_{p}x_{p}}\n",
    "$$\n",
    "\n",
    "\\hat{p} is the expected probability that the outcome is present; x{1} through x{p} are distinct independent variables; and \\beta{0} through \\beta{p} are the regression coefficients. The multiple logistic regression model is sometimes written differently. In the following form, the outcome is the expected log of the odds that the outcome is present,\n",
    "\n",
    "$$ \\ln{\\frac{\\hat{p}}{1-\\hat{p}}}=\\beta_{0} +\\beta_{1}x_{1}+\\beta_{2}x_{2}+...+\\beta_{p}x_{p} $$\n",
    "\n",
    "When analyzing a polytomous response, it's important to note whether the response is ordinal (consisting of ordered categories) or nominal (consisting of unordered categories). For binary logistic model this question does not arise.\n",
    "\n",
    "Some types of models are appropriate only for ordinal responses; e.g., cumulative logits model, adjacent categories model, continuation ratios model and other models may be used whether the response is ordinal or nominal; e.g., baseline logit model, and conditional logit model.\n",
    "\n",
    "If the response is ordinal, we do not necessarily have to take the ordering into account, but only very rarely this information is ignored. Ordinality in the response is a vital information; neglecting it almost always will lead to sub-optimal models. Using the natural ordering can\n",
    "\n",
    "- lead to a simpler, more parsimonious model and\n",
    "- increase power to detect relationships with other variables.\n",
    "\n",
    "If the response variable is polytomous and all the potential predictors are discrete as well, we could describe the multi-way contingency table by a *loglinear model.* However, if you are analyzing a set of categorical variables, and one of them is clearly a \"response\" while the others are predictors, I recommend that you use logistic rather than loglinear models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the Logistic Regression Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train and test values: (3295, 39) (824, 39) (3295,) (824,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state=0)\n",
    "print(\"train and test values:\",X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We divide the data to constitute out model and we divide 20% of the data randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "model=logreg.fit(X_train, y_train)\n",
    "predictions = logreg.predict(X_test)\n",
    "print (model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression model is formed by using *LogisticRegression()* from the library *sklearn* and then we construct the model with *fit()* and *predict()* code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[721  17]\n",
      " [ 54  32]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, predictions)\n",
    "print(\"Confusion Matrix: \")\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix we found is seen above. From the first row which is correct predictions, **738 customers** in the test dataset were not buying deposits (no answer); *721 of them are not bought (no) and 17 are bought (yes)*. \n",
    "\n",
    "From the second row which is incorrect predictions, **86 customers** in the train dataset were not buying deposits (no answer); *54 of them are not bought (no) and 32 are bought (yes).*\n",
    "\n",
    "721 + 32 =753  samples were correctly estimated from *824 sampled dataset* and the success rate was **91.38%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.98      0.95       738\n",
      "          1       0.65      0.37      0.47        86\n",
      "\n",
      "avg / total       0.90      0.91      0.90       824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the entire test set, 90% of the promoted term deposit were the term deposit that the customers liked. Of the entire test set, 91% of the customerâs preferred term deposits that were promoted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression score= 0.91383495\n"
     ]
    }
   ],
   "source": [
    "logreg_score = logreg.predict(X_test)\n",
    "print('Accuracy of logistic regression score= {:.8f}'.format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of logistic regression score is consistent the classification report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "logit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the receiver operating characteristic (ROC) curve=  0.674528896452\n"
     ]
    }
   ],
   "source": [
    "print(\"Area under the receiver operating characteristic (ROC) curve= \",logit_roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYYwxJqj/B6rTs64rzMW+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113a9c908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The receiver operating characteristic (ROC) curve is another common tool used with binary classifiers. The dotted line represents the ROC curve of a purely random classifier; a good classifier stays as far away from that line as possible (toward the top-left corner)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation of Columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>...</th>\n",
       "      <th>month_8</th>\n",
       "      <th>month_9</th>\n",
       "      <th>day_of_week_0</th>\n",
       "      <th>day_of_week_1</th>\n",
       "      <th>day_of_week_2</th>\n",
       "      <th>day_of_week_3</th>\n",
       "      <th>day_of_week_4</th>\n",
       "      <th>poutcome_0</th>\n",
       "      <th>poutcome_1</th>\n",
       "      <th>poutcome_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.028246</td>\n",
       "      <td>-0.008091</td>\n",
       "      <td>-0.014204</td>\n",
       "      <td>0.007179</td>\n",
       "      <td>0.013072</td>\n",
       "      <td>0.015529</td>\n",
       "      <td>0.086464</td>\n",
       "      <td>0.029247</td>\n",
       "      <td>0.009961</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023714</td>\n",
       "      <td>0.015196</td>\n",
       "      <td>0.002812</td>\n",
       "      <td>0.024298</td>\n",
       "      <td>-0.008800</td>\n",
       "      <td>0.044226</td>\n",
       "      <td>-0.063853</td>\n",
       "      <td>-0.002842</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>0.017587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>duration</th>\n",
       "      <td>0.028246</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.098310</td>\n",
       "      <td>-0.080311</td>\n",
       "      <td>0.056920</td>\n",
       "      <td>-0.068944</td>\n",
       "      <td>0.023705</td>\n",
       "      <td>-0.029160</td>\n",
       "      <td>-0.069410</td>\n",
       "      <td>-0.095347</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005613</td>\n",
       "      <td>0.024079</td>\n",
       "      <td>-0.015588</td>\n",
       "      <td>-0.021946</td>\n",
       "      <td>0.012390</td>\n",
       "      <td>0.017931</td>\n",
       "      <td>0.006862</td>\n",
       "      <td>0.015180</td>\n",
       "      <td>-0.055316</td>\n",
       "      <td>0.080605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>campaign</th>\n",
       "      <td>-0.008091</td>\n",
       "      <td>-0.098310</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.059960</td>\n",
       "      <td>-0.097753</td>\n",
       "      <td>0.173279</td>\n",
       "      <td>0.128599</td>\n",
       "      <td>0.014350</td>\n",
       "      <td>0.128145</td>\n",
       "      <td>0.153976</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.066038</td>\n",
       "      <td>-0.044614</td>\n",
       "      <td>0.040080</td>\n",
       "      <td>0.056928</td>\n",
       "      <td>-0.046224</td>\n",
       "      <td>-0.008625</td>\n",
       "      <td>-0.041640</td>\n",
       "      <td>-0.076579</td>\n",
       "      <td>0.096873</td>\n",
       "      <td>-0.055344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pdays</th>\n",
       "      <td>-0.014204</td>\n",
       "      <td>-0.080311</td>\n",
       "      <td>0.059960</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.507412</td>\n",
       "      <td>0.227693</td>\n",
       "      <td>0.013684</td>\n",
       "      <td>-0.083513</td>\n",
       "      <td>0.233311</td>\n",
       "      <td>0.296307</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.070393</td>\n",
       "      <td>-0.188869</td>\n",
       "      <td>0.003055</td>\n",
       "      <td>-0.011833</td>\n",
       "      <td>-0.001807</td>\n",
       "      <td>-0.016964</td>\n",
       "      <td>0.028334</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.488644</td>\n",
       "      <td>-0.942457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>previous</th>\n",
       "      <td>0.007179</td>\n",
       "      <td>0.056920</td>\n",
       "      <td>-0.097753</td>\n",
       "      <td>-0.507412</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.445049</td>\n",
       "      <td>-0.233331</td>\n",
       "      <td>-0.115118</td>\n",
       "      <td>-0.389316</td>\n",
       "      <td>-0.452448</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096377</td>\n",
       "      <td>0.153216</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>0.000771</td>\n",
       "      <td>0.001397</td>\n",
       "      <td>-0.001433</td>\n",
       "      <td>-0.012657</td>\n",
       "      <td>0.846434</td>\n",
       "      <td>-0.998001</td>\n",
       "      <td>0.471469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emp.var.rate</th>\n",
       "      <td>0.013072</td>\n",
       "      <td>-0.068944</td>\n",
       "      <td>0.173279</td>\n",
       "      <td>0.227693</td>\n",
       "      <td>-0.445049</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.575597</td>\n",
       "      <td>0.228463</td>\n",
       "      <td>0.796372</td>\n",
       "      <td>0.940285</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.173401</td>\n",
       "      <td>-0.137221</td>\n",
       "      <td>-0.018711</td>\n",
       "      <td>-0.019572</td>\n",
       "      <td>0.011872</td>\n",
       "      <td>0.018649</td>\n",
       "      <td>0.007305</td>\n",
       "      <td>-0.375434</td>\n",
       "      <td>0.446153</td>\n",
       "      <td>-0.215852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cons.price.idx</th>\n",
       "      <td>0.015529</td>\n",
       "      <td>0.023705</td>\n",
       "      <td>0.128599</td>\n",
       "      <td>0.013684</td>\n",
       "      <td>-0.233331</td>\n",
       "      <td>0.575597</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200624</td>\n",
       "      <td>0.355936</td>\n",
       "      <td>0.313130</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042026</td>\n",
       "      <td>0.033672</td>\n",
       "      <td>0.025207</td>\n",
       "      <td>0.012180</td>\n",
       "      <td>-0.036331</td>\n",
       "      <td>0.008961</td>\n",
       "      <td>-0.009130</td>\n",
       "      <td>-0.262834</td>\n",
       "      <td>0.244207</td>\n",
       "      <td>-0.019733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <td>0.086464</td>\n",
       "      <td>-0.029160</td>\n",
       "      <td>0.014350</td>\n",
       "      <td>-0.083513</td>\n",
       "      <td>-0.115118</td>\n",
       "      <td>0.228463</td>\n",
       "      <td>0.200624</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.237896</td>\n",
       "      <td>0.134503</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080015</td>\n",
       "      <td>0.116128</td>\n",
       "      <td>-0.001795</td>\n",
       "      <td>-0.039329</td>\n",
       "      <td>-0.024647</td>\n",
       "      <td>0.066625</td>\n",
       "      <td>-0.000479</td>\n",
       "      <td>-0.183737</td>\n",
       "      <td>0.121482</td>\n",
       "      <td>0.081139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>euribor3m</th>\n",
       "      <td>0.029247</td>\n",
       "      <td>-0.069410</td>\n",
       "      <td>0.128145</td>\n",
       "      <td>0.233311</td>\n",
       "      <td>-0.389316</td>\n",
       "      <td>0.796372</td>\n",
       "      <td>0.355936</td>\n",
       "      <td>0.237896</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.778794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.141886</td>\n",
       "      <td>-0.147038</td>\n",
       "      <td>0.046998</td>\n",
       "      <td>-0.132488</td>\n",
       "      <td>-0.013106</td>\n",
       "      <td>0.085449</td>\n",
       "      <td>0.015992</td>\n",
       "      <td>-0.307005</td>\n",
       "      <td>0.387323</td>\n",
       "      <td>-0.219871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nr.employed</th>\n",
       "      <td>0.009961</td>\n",
       "      <td>-0.095347</td>\n",
       "      <td>0.153976</td>\n",
       "      <td>0.296307</td>\n",
       "      <td>-0.452448</td>\n",
       "      <td>0.940285</td>\n",
       "      <td>0.313130</td>\n",
       "      <td>0.134503</td>\n",
       "      <td>0.778794</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.191010</td>\n",
       "      <td>-0.211561</td>\n",
       "      <td>-0.020874</td>\n",
       "      <td>-0.024064</td>\n",
       "      <td>0.025956</td>\n",
       "      <td>0.008917</td>\n",
       "      <td>0.009490</td>\n",
       "      <td>-0.341643</td>\n",
       "      <td>0.447520</td>\n",
       "      <td>-0.276489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     age  duration  campaign     pdays  previous  \\\n",
       "age             1.000000  0.028246 -0.008091 -0.014204  0.007179   \n",
       "duration        0.028246  1.000000 -0.098310 -0.080311  0.056920   \n",
       "campaign       -0.008091 -0.098310  1.000000  0.059960 -0.097753   \n",
       "pdays          -0.014204 -0.080311  0.059960  1.000000 -0.507412   \n",
       "previous        0.007179  0.056920 -0.097753 -0.507412  1.000000   \n",
       "emp.var.rate    0.013072 -0.068944  0.173279  0.227693 -0.445049   \n",
       "cons.price.idx  0.015529  0.023705  0.128599  0.013684 -0.233331   \n",
       "cons.conf.idx   0.086464 -0.029160  0.014350 -0.083513 -0.115118   \n",
       "euribor3m       0.029247 -0.069410  0.128145  0.233311 -0.389316   \n",
       "nr.employed     0.009961 -0.095347  0.153976  0.296307 -0.452448   \n",
       "\n",
       "                emp.var.rate  cons.price.idx  cons.conf.idx  euribor3m  \\\n",
       "age                 0.013072        0.015529       0.086464   0.029247   \n",
       "duration           -0.068944        0.023705      -0.029160  -0.069410   \n",
       "campaign            0.173279        0.128599       0.014350   0.128145   \n",
       "pdays               0.227693        0.013684      -0.083513   0.233311   \n",
       "previous           -0.445049       -0.233331      -0.115118  -0.389316   \n",
       "emp.var.rate        1.000000        0.575597       0.228463   0.796372   \n",
       "cons.price.idx      0.575597        1.000000       0.200624   0.355936   \n",
       "cons.conf.idx       0.228463        0.200624       1.000000   0.237896   \n",
       "euribor3m           0.796372        0.355936       0.237896   1.000000   \n",
       "nr.employed         0.940285        0.313130       0.134503   0.778794   \n",
       "\n",
       "                nr.employed     ...       month_8   month_9  day_of_week_0  \\\n",
       "age                0.009961     ...      0.023714  0.015196       0.002812   \n",
       "duration          -0.095347     ...     -0.005613  0.024079      -0.015588   \n",
       "campaign           0.153976     ...     -0.066038 -0.044614       0.040080   \n",
       "pdays              0.296307     ...     -0.070393 -0.188869       0.003055   \n",
       "previous          -0.452448     ...      0.096377  0.153216       0.012048   \n",
       "emp.var.rate       0.940285     ...     -0.173401 -0.137221      -0.018711   \n",
       "cons.price.idx     0.313130     ...     -0.042026  0.033672       0.025207   \n",
       "cons.conf.idx      0.134503     ...      0.080015  0.116128      -0.001795   \n",
       "euribor3m          0.778794     ...     -0.141886 -0.147038       0.046998   \n",
       "nr.employed        1.000000     ...     -0.191010 -0.211561      -0.020874   \n",
       "\n",
       "                day_of_week_1  day_of_week_2  day_of_week_3  day_of_week_4  \\\n",
       "age                  0.024298      -0.008800       0.044226      -0.063853   \n",
       "duration            -0.021946       0.012390       0.017931       0.006862   \n",
       "campaign             0.056928      -0.046224      -0.008625      -0.041640   \n",
       "pdays               -0.011833      -0.001807      -0.016964       0.028334   \n",
       "previous             0.000771       0.001397      -0.001433      -0.012657   \n",
       "emp.var.rate        -0.019572       0.011872       0.018649       0.007305   \n",
       "cons.price.idx       0.012180      -0.036331       0.008961      -0.009130   \n",
       "cons.conf.idx       -0.039329      -0.024647       0.066625      -0.000479   \n",
       "euribor3m           -0.132488      -0.013106       0.085449       0.015992   \n",
       "nr.employed         -0.024064       0.025956       0.008917       0.009490   \n",
       "\n",
       "                poutcome_0  poutcome_1  poutcome_2  \n",
       "age              -0.002842   -0.006591    0.017587  \n",
       "duration          0.015180   -0.055316    0.080605  \n",
       "campaign         -0.076579    0.096873   -0.055344  \n",
       "pdays             0.000140    0.488644   -0.942457  \n",
       "previous          0.846434   -0.998001    0.471469  \n",
       "emp.var.rate     -0.375434    0.446153   -0.215852  \n",
       "cons.price.idx   -0.262834    0.244207   -0.019733  \n",
       "cons.conf.idx    -0.183737    0.121482    0.081139  \n",
       "euribor3m        -0.307005    0.387323   -0.219871  \n",
       "nr.employed      -0.341643    0.447520   -0.276489  \n",
       "\n",
       "[10 rows x 39 columns]"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Correlation of Columns\")\n",
    "dataset.corr(method='spearman').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table include correlation numbers of first ten columns. We look at the table and delete some columns which have close correlation coefficients. \n",
    "\n",
    "For example, the variables *emp.var.rate, euribor3m  and nr.employed* have close  correlation coefficient, we use the strongest of them (nr.employed) to make the method more effective. And we simplify the data by making the following groupings on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78643631144366166"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['emp.var.rate'].corr(dataset['euribor3m'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75331918713177837"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['nr.employed'].corr(dataset['euribor3m'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset=pd.read_csv('Data-main.csv',header=0)\n",
    "dataset=dataset.dropna()\n",
    "\n",
    "dataset.drop(\"euribor3m\", axis=1, inplace=True)\n",
    "dataset.drop(\"emp.var.rate\", axis=1, inplace=True)\n",
    "\n",
    "dataset['education']=np.where(dataset['education'] =='basic.9y', 'Basic', dataset['education'])\n",
    "dataset['education']=np.where(dataset['education'] =='basic.6y', 'Basic', dataset['education'])\n",
    "dataset['education']=np.where(dataset['education'] =='basic.4y', 'Basic', dataset['education'])\n",
    "\n",
    "dataset['marital']=np.where(dataset['marital'] =='divorced', 'Single', dataset['marital'])\n",
    "dataset['marital']=np.where(dataset['marital'] =='single', 'Single', dataset['marital'])\n",
    "\n",
    "dataset['job']=np.where(dataset['job'] =='admin.', 'High', dataset['job'])\n",
    "dataset['job']=np.where(dataset['job'] =='management', 'High', dataset['job'])\n",
    "dataset['job']=np.where(dataset['job'] =='blue-collar', 'Basic', dataset['job'])\n",
    "dataset['job']=np.where(dataset['job'] =='services', 'Basic', dataset['job'])\n",
    "dataset['job']=np.where(dataset['job'] =='housemaid', 'Basic', dataset['job'])\n",
    "dataset['job']=np.where(dataset['job'] =='technician', 'Basic', dataset['job'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"divorcedâ and âsingleâ  grouped together and call them âsingleâ.\n",
    "\n",
    "âbasic.4yâ, âbasic.9yâ and âbasic.6yâ grouped together and call them âbasicâ.\n",
    "\n",
    "âadmin.â, âmanagementâ and âentrepreneurâ grouped together and call them 'high'; \"blue-collar\", \"technician\", \"services\" and \"housemaid\" grouped together and call them 'basic'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train and test values: (3295, 39) (824, 39) (3295,) (824,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state=0)\n",
    "print(\"train and test values:\",X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "model=logreg.fit(X_train, y_train)\n",
    "predictions = logreg.predict(X_test)\n",
    "print (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[721  17]\n",
      " [ 54  32]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, predictions)\n",
    "print(\"Confusion Matrix: \")\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the receiver operating characteristic (ROC) curve=  0.674528896452\n"
     ]
    }
   ],
   "source": [
    "print(\"Area under the receiver operating characteristic (ROC) curve= \",logit_roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same results before grouping. It did not change the end result of grouping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUPPORT VECTOR MACHINE\n",
    "\n",
    "#### Introduction to Classification\n",
    "\n",
    "Support Vector Machine (SVM) is an audited machine learning algorithm that can be used for classification or regression difficulties, but is often used in classification problems.\n",
    "\n",
    "The classification problem can be restricted to consideration of the two-class problem without loss of generality. The goal is to separate the two classes by a function which is induced from available examples and it is to produce a classifier that will work well on unseen examples, i.e. it generalises well.\n",
    "\n",
    "\n",
    "A Support Vector Machine (SVM) performs classification by finding the hyperplane that maximizes the margin between the two classes. The vectors (cases) that define the hyperplane are the support vectors.\n",
    "\n",
    "![](http://www.saedsayad.com/images/SVM_2.png)$ [3] $\n",
    "\n",
    "  \n",
    " #### Algorithm:\n",
    "\n",
    "- Define an optimal hyperplane: maximize margin\n",
    "- Extend the above definition for non-linearly separable problems: have a penalty term for misclassifications.\n",
    "- Map data to high dimensional space where it is easier to classify with linear decision surfaces: reformulate problem so that data is mapped implicitly to this space.\n",
    "- To define an optimal hyperplane we need to maximize the width of the margin (w).\n",
    "\n",
    "![]( https://s3.amazonaws.com/quantstart/media/images/qs-svm-0003.png ) [4]\n",
    "\n",
    "\n",
    "Consider the problem of separating the set of training vectors belonging to two separate classes,\n",
    "\n",
    " $$ D= { (x^{1},y^{1}),..., (x^{k},y^{k}) } $$\n",
    " \n",
    " where  $ x \\epsilon R $  and  $ y \\epsilon {-1,1} $   with a hyperplane $ < w,x> +b=0 $\n",
    "\n",
    "The set of vectors is said to be optimally separated by the hyperplane if it is separated without error and the distance between the closest vector to the hyperplane is maximal. \n",
    "\n",
    "$$ min| <w,x^{i}>+b | =1 \\\\ $$\n",
    "\n",
    "This incisive constraint on the parameterisation is preferable to alternatives in simpli- fying the formulation of the problem. In words it states that: the norm of the weight vector should be equal to the inverse of the distance, of the nearest point in the data set to the hyperplane.\n",
    "\n",
    " SVMs find a hyperplane $ w Â· x + b = 0 $ which correctly separates training examples and has maximum margin which is the distance between two hyperplanes $ w Â· x + b â¥ 1 $ and $ w Â· x + b â¤ â1 $ \n",
    " \n",
    "A separating hyperplane in canonical form must satisfy the following constraints,\n",
    "\n",
    "$$ y^{i} [<w,x^{i}>+b]\\geq 1 $$ where i=1,...,k $ (*) $\n",
    "\n",
    "The distance d(w, b; x) of a point x from the hyperplane (w, b) is,\n",
    "$$ d(w,b;x)=\\frac{| <w,x^{i}>+b |}{\\parallel w \\parallel} $$ \n",
    "\n",
    "The optimal hyperplane is given by maximising the margin, Ï, subject to the constraints of $ (*) $. The margin is given by,\n",
    "\n",
    "$$  Ï (w,b)= min_{x^{i}:y^{i}=-1} d(w, b; x^{i}) + min_{x^{i}:y^{i}=1} d(w, b; x^{i})=\\frac {2}{\\parallel w \\parallel} $$\n",
    "\n",
    "Hence the hyperplane that optimally separates the data is the one that minimises\n",
    "\n",
    "$$ \\phi (w)=\\frac{1}{2} {\\parallel w \\parallel} ^2 $$ \n",
    "$ (**)$\n",
    "\n",
    "The solution to the optimisation problem of $ (**) $ under the constraints $ (*) $ is given by the saddle point of the Lagrange functional \n",
    "$$ \\phi (w,b,\\alpha)=\\frac{1}{2} {\\parallel w \\parallel} ^2 - \\sum_i^l \\alpha_{i} (y^{i} [<w,x^{i}>+b]-1) $$ , where i=1,2,...l\n",
    "and $ \\alpha $ are the Lagrange Multipliers. The Lagrangian has to be minimised with to w,b and maximised with respect to $ \\alpha $  \\geq 0. \n",
    "\n",
    "The minimum with respect to w and b of the Lagrangian, \\phi, is given by, \n",
    "$$  \\frac{\\partial \\phi}{\\partial b}=0  \\Longrightarrow \\sum_i^l \\alpha_{i} y_{i}=0 $$  \n",
    "\n",
    "$$  \\frac{\\partial \\phi}{\\partial w}=0 \\Longrightarrow w=\\sum_i^l \\alpha_{i} y_{i} x_{i} $$  \n",
    "\n",
    "When we rewrite the minimum Lagrangian by putting w and $$b=\\sum_i^l \\alpha_{i} y_{i}=0 $$ then we have the maximum Lagrangian $$ maxL=\\sum_i^l\\alpha_{i}-\\frac{1}{2}\\sum_i^l \\alpha_{i} \\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) $$ \n",
    "\n",
    "where $ \\alpha_{i} $'s are support vectors and positive. \n",
    " \n",
    "> The simplest way to separate two groups of data is with a straight line (1 dimension), flat plane (2 dimensions) or an N-dimensional hyperplane. However, there are situations where a nonlinear region can separate the groups more efficiently. SVM handles this by using a kernel function (nonlinear) to map the data into a different space where a hyperplane (linear) cannot be used to do the separation. It means a non-linear function is learned by a linear learning machine in a high-dimensional feature space while the capacity of the system is controlled by a parameter that does not depend on the dimensionality of the space. This is called kernel trick which means the kernel function transform the data into a higher dimensional feature space to make it possible to perform the linear separation. [3]\n",
    " \n",
    "$$ SVM =\\begin{cases}Linear SVM & x_{i}.x_{j}\\\\ Non-linear SVM & \\phi(x_{i}).\\phi(x_{j})\\\\Kernel Function & k(x_{i}.x_{j})\n",
    " \\end{cases}  $$\n",
    "\n",
    "\n",
    "The beauty of SVM is that if the data is linearly separable, there is a unique global minimum value. An ideal SVM analysis should produce a hyperplane that completely separates the vectors (cases) into two non-overlapping classes. However, perfect separation may not be possible, or it may result in a model with so many cases that the model does not classify correctly. In this situation SVM finds the hyperplane that maximizes the margin and minimizes the misclassifications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the Support Vector Machine Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and Test Values: (3295, 39) (824, 39) (3295,) (824,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "print(\"Train and Test Values:\",X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "sc = StandardScaler()\n",
    "X_train_scaled = sc.fit_transform(X_train)\n",
    "X_test_scaled = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We divide the data to constitute out model and we divide 20% of the data randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'linear')\n",
    "classifier.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine model is formed by using *classifier()*  from the library *sklearn* and then we construct the model with *fit()* code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[720  18]\n",
      " [ 56  30]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = classifier.predict(X_test_scaled)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix: \")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The confusion matrix we found is seen above. From the first row, **738 customers** in the test dataset were not buying deposits (no answer); 720 of them are not bought (no) and 18 are bought (yes). \n",
    "\n",
    "- From the second row, **86 customers** in the train dataset were not buying deposits (no answer); 56 of them are not bought (no) and 30 are bought (yes).\n",
    "\n",
    "- 720 + 30 = 750 samples were correctly estimated from 824 sampled dataset and the success rate was **91.01%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.98      0.95       738\n",
      "          1       0.62      0.35      0.45        86\n",
      "\n",
      "avg / total       0.90      0.91      0.90       824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the entire test set, 90% of the promoted term deposit were the term deposit that the customers liked. Of the entire test set, 91% of the customerâs preferred term deposits that were promoted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine Score= 0.910194174757\n"
     ]
    }
   ],
   "source": [
    "print(\"Support Vector Machine Score=\",classifier.score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The support vector machine score is consistent the classification report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEWCAYAAABmE+CbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niAAKCCIiElJAEBERQAFBRERC/x8a2XpSw0aBIgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1d9a2ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = X_test_scaled, y_test\n",
    "\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
    "\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "             \n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
    "                c = ListedColormap(('red', 'green'))(i), label = j)\n",
    "plt.title('SVM Test Set')\n",
    "plt.xlabel('X values')\n",
    "plt.ylabel('y Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Result\n",
    "\n",
    "When we compare the results obtained by applying logistic regression and support vector machine algorithms to the bank data set; in both algorithms 80% of the data set was used to construct the model while 20% of the data set was used in the test phase. The logistic regression algorithm with 91.38% accuracy and the support vector machine algorithm with 91.01% were found when the accuracy ratios of the predictions of confusion matrix were calculated. It is obvious that quite close to each other as seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REFERENCES\n",
    "\n",
    "\n",
    "[1] http://archive.ics.uci.edu/ml/datasets/Bank+Marketing\n",
    "\n",
    "[2] http://www.saedsayad.com/logistic_regression.htm \n",
    "\n",
    "[3] http://www.saedsayad.com/images/SVM_2.png\n",
    "\n",
    "[4] https://s3.amazonaws.com/quantstart/media/images/qs-svm-0003.png \n",
    "\n",
    "[5] David W. Hosmer, Stanley Lemeshow ,*Logistic Regression*, http://resource.heartonline.cn/20150528/1_3kOQSTg.pdf \n",
    "\n",
    "[6] Penn Stage Eberly College of Science, https://onlinecourses.science.psu.edu/stat504/node/172 [online]\n",
    "\n",
    "[7] Hiroyasu Yamada, Yuji Matsumoto *STATISTICAL DEPENDENCY ANALYSIS WITH SUPPORT VECTOR MACHINES*,http://www.jaist.jp/~h-yamada/pdf/iwpt2003.pdf [online]\n",
    "\n",
    "[8] Gunn, Steve R. *Support Vector Machines for Classification and Regression*,10 May 1998\n",
    "\n",
    "[9] http://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_multivariable/BS704_Multivariable8.html [online]\n",
    "\n",
    "[10] Penn Stage Eberly College of Science,\n",
    "http://www.statsoft.com/Textbook/Support-Vector-Machines#overview [online]\n",
    "\n",
    "[11] http://www.saedsayad.com/support_vector_machine.htm [online]\n",
    "\n",
    "[12] http://www.statsoft.com/Textbook/Support-Vector-Machines#overview [online]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
