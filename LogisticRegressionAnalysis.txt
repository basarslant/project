## LOGISTIC REGRESSION


#### *Introduction to Logistic Regression Model*

   Before beginning a study of logistic regression it is important to understand that the goal of an analysis using this method is the same as that of any model-building technique used in statistics: to find the best fitting and most parsimonious, yet biologically reasonable model to describe the relationship between an outcome variable which is dependent or response and a set of independent variables which is predictor or explanatory. These independent variables are called **covariates**.

   In any regression problem the key quantity is the mean value of the outcome variable, given the value of the independent variable which is called **conditional mean** and it is expressed as **"E(Y|x)"** where *Y* denotes the outcome variable and *x* denotes a value of the independent variable. E(Y|x) is read *"the expected value of Y, given the value x."* In linear regression we assume that this mean may be expressed as an euation linear in x ( or some transformation of *x* or *Y*), such as ***E(Y|x)=\beta_{0} +\beta_{1}x***. 
This expression implies that it is possible for E(Y|x) to take on any value as x ranges between -\infty and +\infty.

   **Logistic Regression Model** is a statistical method for analyzing a dataset in which are *dichotomous(binary)* independent variables that determine an outcome. 

What distinguishes a logistic regression from the linear regression model is this. The prediction is based on the use of one or several predictors (numerical and categorical). A linear regression is not appropriate for predicting the value of a binary variable for two reasons:

- A linear regression will predict values outside the acceptable range (e.g. predicting probabilities outside the range 0 to 1)
- Since the dichotomous experiments can only have one of two possible values for each experiment, the residuals will not be normally distributed about the predicted line.
                                                                                       
![](http://www.saedsayad.com/images/LogReg_1.png)

 >On the other hand, a logistic regression produces a logistic curve, which is limited to values between 0 and 1.
 
 >In the logistic regression the constant (b0) moves the curve left and right and the slope (b1) defines the steepness of the curve. By simple transformation, the logistic regression equation can be written in terms of an odds ratio.

>Finally, taking the natural log of both sides, we can write the equation in terms of log-odds (logit) which is a linear function of the predictors. The coefficient (b1) is the amount the logit (log-odds) changes with a one unit change in x. 


#### *Interpretation of Fitting Logistic Regression Analysis*

   The first step is to determine what function of the dependent variable yields a linear function of the independent variables. This is called the ***link function*** [see McCullagh and Nelder(1983) or Dobson(1990)]. In this case of a linear regression model, it is the identity function since the dependent variable, by definition, is linear in the parameters. In the logistic regression model the link function is the logit transformation

\begin{center}
g(x)=\ln{\frac{\pi(x)}{1-\pi(x)}}=\beta_{0} +\beta_{1}x 
\end{center}

   In the regression model, the slope coefficient represents the change in the logit corresponding to a change of one unit in the independent variable (i.e. **\beta_{1}= g(x+1)-g(x)**). Proper interpretation of the coefficient in a logistic regression model depends on being able to place meaning on the difference between two logits. Interpretation of this difference is discussed in detail on a case-by-case basis as it relates directly to the definition and meaning of a one-unit change in the independent variable. In the following sections of this chapter we consider the interpretation of the coefficients for a univariate logistic regression model for each of the possible measurement scales of the independent variable. In addition we discuss interpretation of the coefficients in multivariable models.


#### Dichotomous Independent Variable
  
   We begin our consideration of the interpretation of logistic regression coefficients with the situation where the independent variable is nominal scale and dichotomous(i.e. measured at two levels). This case provides the conceptual foundation for all the other situations.

   We assume that the independent variable, *x*, is coded as either zero or one. The difference in the logit for a subject with x=1 and x=0 is
   
\begin{center}
g(1)-g(0)=[\beta_{0} +\beta_{1}x]- \beta_{0}=\beta_{1}.
\end{center}

   The algebra shown in this equation is rather straightforward. We present it in this level of detail to emphasize that the fisrt step in interpreting the effect of a covariate in a model is to express the desired logit difference in terms of the model. In this case the logit difference is equal to \beta_{1}. In order to interpret this result we need to introduce and discuss a measure of association termed the *odds ratio*.
	 
   The *odds* of the outcome being present among individuals with x=1 is defined as \frac{\pi(1)}{1-\pi(1)}. Similarly the odds of the outcome being present among individuals with x=0 is defined as \frac{\pi(0)}{1-\pi(0)}. The **odds ratio** is defined as the ratio of the odds for x=1 to the *odds* for x=0 and is given by the equation
   
\begin{center}
Odds Ratio={\frac{\frac{\pi(1)}{1-\pi(1)}}{\frac{\pi(0)}{1-\pi(0)}}}}
\end{center}


*************** sayfa 49-50 resim

  
 Hence for logistic regression with a dichotomous independent variable coded 1 and 0, the relationship between the odds ratio and the regression coefficient is *Odds Ratio= \mathrm{e}^{\beta_{1}}*.

 This simple relationship between the coefficient and the odds ratio is the fundamental reason why logistic regression has proven to be such a powerful analytic research tool.

 The interpretation given for the odds ratio is based on the fact that in many instances it approximates a quantity called **the relative risk**. This parameter is equal to the ratio \frac{\pi(1)}{\pi(0)}. It follows that the odds ratio approximates the relative risk \frac{1-\pi(0)}{1-\pi(1)}\approx 1.
This holds when \pi(x) is small for both x=1 and x=0.


#### *Polytomous (Multinomial) Logistic Regression Analysis* 

   Where the response is a binary variable with 'success' and 'failure' being only two categories. But logistic regression can be extended to handle responses, Y, that are polytomous, i.e. taking r > 2 categories. 
   
   When r = 2, Y is dichotomous and we can model log of odds that an event occurs or does not occur. For binary logistic regression there is only 1 logit that we can form.
   
   **logit(\pi)=\log{\frac{\pi(x)}{1-\pi(x)}}**
   
   When r > 2, we have a multi-category or polytomous response variable. There are *r (r − 1)/2 logits (odds)* that we can form, but only *(r − 1)* are non-redundant. There are different ways to form a set of (r − 1) non-redundant logits, and these will lead to different polytomous (multinomial) logistic regression models.
   
  Multinomial Logistic Regression models how multinomial response variable Y depends on a set of k explanatory variables x=(x_{1},x_{2},...,x_{p}). This is also a Generalized Linear Model where the random component assumes that the distribution of Y is  Multinomial(n,π), where π is a vector with probabilities of *"success"* for each category. The systematic component are explanatory variables (can be continuous, discrete, or both) and are linear in the parameters, e.g., \beta_{0} +\beta_{1}x_{1}+\beta_{2}x_{2}+...+\beta_{p}x_{p}. Again, transformation of the X's themselves are allowed like in linear regression. The *link function* is the generalized **Logit**, that it the logit link for each pair of non-redundant logits as discussed above. 
  
  For the moment we will assume that each of these variables is at least interval scale. Let the conditional probability that the outcome is present be denoted by P(Y=1|x)= \pi(x). The ***logit*** of the multiple logistic regression model is given by the equation 
                                                                                    \begin{center}
**g(x)=\beta_{0} +\beta_{1}x_{1}+\beta_{2}x_{2}+...+\beta_{p}x_{p}**, 
\end{center}

in which case the logistic regression model is

\begin{center}
\pi(x)=\frac{\mathrm{e}^{g(x)}}{1+\mathrm{e}^{g(x)}}
\end{center}

The multiple logistic regression model can be written as follows:

\begin{center}
\hat{p} = \frac{\beta_{0} +\beta_{1}x_{1}+\beta_{2}x_{2}+...+\beta_{p}x_{p}}{1-\beta_{0} +\beta_{1}x_{1}+\beta_{2}x_{2}+...+\beta_{p}x_{p}}
\end{center}

\hat{p} is the expected probability that the outcome is present; x_{1} through x_{p} are distinct independent variables; and \beta_{0} through \beta_{p} are the regression coefficients. The multiple logistic regression model is sometimes written differently. In the following form, the outcome is the expected log of the odds that the outcome is present,

\ln{\frac{\hat{p}}{1-\hat{p}}}=\beta_{0} +\beta_{1}x_{1}+\beta_{2}x_{2}+...+\beta_{p}x_{p}


 When analyzing a polytomous response, it's important to note whether the response is **ordinal** (consisting of ordered categories) or **nominal** (consisting of unordered categories). For binary logistic model this question does not arise.

 Some types of models are appropriate only for ordinal responses; e.g., *cumulative logits model, adjacent categories model, continuation ratios model* and other models may be used whether the response is ordinal or nominal; e.g., baseline *logit model, and conditional logit model.*

 If the response is *ordinal*, we do not necessarily have to take the ordering into account, but only very rarely this information is ignored. Ordinality in the response is a vital information; neglecting it almost always will lead to sub-optimal models. Using the natural ordering can

- lead to a simpler, more parsimonious model and
- increase power to detect relationships with other variables.

 If the response variable is polytomous and all the potential predictors are discrete as well, we could describe the multi-way contingency table by a *loglinear model*. However, if you are analyzing a set of categorical variables, and one of them is clearly a "response" while the others are predictors, I recommend that you use logistic rather than loglinear models. 

***Fitting a loglinear model in this setting could have two disadvantages:***

- It has many more parameters, and many of them are not of interest. The loglinear model, as we will learn later, describes the joint distribution of all the variables, whereas the logistic model describes only the conditional distribution of the response given the predictors.
- The loglinear model is often more complicated to interpret. In the loglinear model, the effect of a predictor X on the response Y is described by the XY association. In a logit model, however, the effect of X on Y is a main effect.


#### ***RESOURCES:***

- http://archive.ics.uci.edu/ml/datasets/Bank+Marketing
- http://resource.heartonline.cn/20150528/1_3kOQSTg.pdf
- https://onlinecourses.science.psu.edu/stat504/node/172
- http://www.saedsayad.com/logistic_regression.htm
- https://www.medcalc.org/manual/logistic_regression.php
- http://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_multivariable/BS704_Multivariable8.html
